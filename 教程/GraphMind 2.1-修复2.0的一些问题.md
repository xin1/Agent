```
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
```
ä»¥ä¸‹æ˜¯å®Œæ•´æ”¯æŒ **å¤š GPU å¹¶è¡Œ** çš„ä»£ç ç‰ˆæœ¬ã€‚æˆ‘ä»¬ä¼šå¯¹ `analyze_docs.py` å’Œ `run.py` ä¸¤ä¸ªæ¨¡å—åšä¿®æ”¹ï¼Œå¹¶ä¿è¯æ¯ä¸ªè¿›ç¨‹è‡ªåŠ¨åˆ†é…ä¸åŒ GPUï¼Œä»¥é¿å…é‡å¤å ç”¨åŒä¸€å—æ˜¾å¡å†…å­˜ã€‚

---

## ğŸ“ ç›®å½•ç»“æ„ï¼ˆä¿æŒä¸€è‡´ï¼‰ï¼š

```
your_project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ extract_text.py
â”‚   â”œâ”€â”€ analyze_docs.py       â† âœ… ä¿®æ”¹
â”‚   â”œâ”€â”€ build_graph.py
â”‚   â”œâ”€â”€ export_dify.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/                 â† å­˜æ”¾ PDF æ–‡æ¡£
â”œâ”€â”€ run.py                    â† âœ… ä¿®æ”¹
```

---

## âœ… `app/analyze_docs.py`ï¼ˆæ”¯æŒ GPU ID è‡ªåŠ¨åˆ†é…ï¼‰

```python
# app/analyze_docs.py
from transformers import AutoTokenizer, AutoModel
import torch

def init_model(gpu_id=0):
    device = f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).half().to(device).eval()
    return tokenizer, model, device

def chunk_text(text, max_len=1500):
    return [text[i:i+max_len] for i in range(0, len(text), max_len)]

def summarize_and_tag_single(args):
    filename, text, gpu_id = args
    tokenizer, model, device = init_model(gpu_id)
    chunks = chunk_text(text)
    combined_summary = ""
    for i, chunk in enumerate(chunks):
        prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æ¡£å†…å®¹å¹¶æå–3-5ä¸ªæ ‡ç­¾ï¼Œè¾“å‡ºæ ¼å¼ï¼šã€æ€»ç»“ã€‘xxxã€æ ‡ç­¾ã€‘xxxï¼š\n{chunk}"
        response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=512)
        combined_summary += f"\nç¬¬{i+1}æ®µï¼š{response}\n"
    return filename, combined_summary

def parse_summary_and_labels(text):
    import re
    summary_match = re.search(r"ã€æ€»ç»“ã€‘(.*?)ã€æ ‡ç­¾ã€‘", text, re.S)
    tags_match = re.findall(r"ã€æ ‡ç­¾ã€‘(.*?)\n?", text, re.S)

    summary = summary_match.group(1).strip() if summary_match else text
    tags = []
    for tag_line in tags_match:
        tags += [t.strip("ï¼š:ï¼Œ, ") for t in tag_line.split("ã€") if t.strip()]
    return summary.strip(), list(set(tags))
```

---

## âœ… `run.py`ï¼ˆè‡ªåŠ¨åˆ†é… GPU ç»™ä¸åŒè¿›ç¨‹ï¼‰

```python
# run.py
# run.py
import os
from app.extract_text import extract_text_from_pdf
from app.analyze_docs import summarize_and_tag_single, parse_summary_and_labels
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
from concurrent.futures import ProcessPoolExecutor
import torch
from tqdm import tqdm

def main():
    pdf_dir = "data/pdfs"
    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(".pdf")]

    # Step 1: æå–æ–‡æœ¬
    pdf_texts = {}
    for file in tqdm(pdf_files, desc="æå–PDFæ–‡æœ¬"):
        text = extract_text_from_pdf(os.path.join(pdf_dir, file))
        pdf_texts[file] = text

    # Step 2: å¤šGPUå¹¶è¡Œè°ƒç”¨
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ–¥ï¸ æ£€æµ‹åˆ° {gpu_count} å— GPUï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç†...")

    task_args = []
    for idx, (fname, text) in enumerate(pdf_texts.items()):
        assigned_gpu = idx % gpu_count
        task_args.append((fname, text, assigned_gpu))

    doc_infos = {}
    with ProcessPoolExecutor(max_workers=gpu_count) as executor:
        results = executor.map(summarize_and_tag_single, task_args)
        for fname, result in tqdm(results, total=len(task_args), desc="åˆ†ææ–‡æ¡£"):
            summary, tags = parse_summary_and_labels(result)
            print(f"ğŸ“„ {fname} æ ‡ç­¾ï¼š{tags}")
            doc_infos[fname] = {"summary": summary, "tags": tags}

    # Step 3: æ„å»ºå›¾è°±
    build_doc_graph(doc_infos)

    # Step 4: å¯¼å‡ºä¸º Dify æ ¼å¼
    export_to_dify_format(doc_infos)

if __name__ == "__main__":
    main()

```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

| äº‹é¡¹    | å†…å®¹                                               |
| ----- | ------------------------------------------------ |
| å¤šå¡ç¯å¢ƒ  | é»˜è®¤è½®æµåˆ†é… GPUï¼Œä¾‹å¦‚æ–‡æ¡£ 0 â†’ GPU0ï¼Œæ–‡æ¡£ 1 â†’ GPU1ï¼Œæ–‡æ¡£ 2 â†’ GPU0 |
| å†…å­˜é™åˆ¶  | æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½ä¸€æ¬¡æ¨¡å‹ï¼Œæ¯ä¸ªæ˜¾å¡çº¦å ç”¨ 13GB æ˜¾å­˜ï¼ˆchatglm3-6b åŠç²¾åº¦ï¼‰    |
| å•å¡ä¹Ÿå…¼å®¹ | å¦‚æœåªæœ‰ä¸€å— GPUï¼Œä»ç„¶æ”¯æŒï¼Œåªæ˜¯ä¸ä¼šçœŸæ­£å¹¶è¡Œ                         |

---

## âœ… æ•ˆæœç¤ºä¾‹ï¼ˆè¾“å‡ºï¼‰ï¼š

```bash
ğŸ–¥ï¸ æ£€æµ‹åˆ° 2 å— GPUï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç†...
åˆ†ææ–‡æ¡£: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:38<00:00,  9.61s/it]
ğŸ“„ doc1.pdf æ ‡ç­¾ï¼š['ç®—æ³•', 'æ·±åº¦å­¦ä¹ ']
ğŸ“„ doc2.pdf æ ‡ç­¾ï¼š['ç”µæœºæ§åˆ¶', 'è‡ªåŠ¨åŒ–']
...
```

---

å¦‚éœ€æ”¯æŒæ˜¾å¡æ‰‹åŠ¨æŒ‡å®šï¼Œæˆ–ç»“åˆ `ray` / `deepspeed` / `accelerate` ä¼˜åŒ–æ¨¡å‹åŠ è½½ï¼Œä¹Ÿå¯ä»¥å†è¿›ä¸€æ­¥å‡çº§ã€‚æ˜¯å¦éœ€è¦æˆ‘æä¾› `ray` å¹¶è¡Œæ¡†æ¶çš„ç‰ˆæœ¬ï¼Ÿ

