ä½ å½“å‰æœ‰ **2 ä¸ªå¯ç”¨ GPU**ï¼ˆæ˜¾å­˜åˆ†åˆ«ä¸º 24GB å’Œ 12GBï¼‰ï¼Œè¿™ä¸ºå¤šæ–‡æ¡£å¹¶è¡Œå¤„ç†æä¾›äº†è‰¯å¥½åŸºç¡€ã€‚ä½†ä¸ºäº†é˜²æ­¢ **çˆ†æ˜¾å­˜** é—®é¢˜ï¼Œå»ºè®®ä»¥ä¸‹ä¸¤ç§å¤„ç†ç­–ç•¥ï¼š

---

### âœ… ä¿®æ”¹æ–¹æ¡ˆæ¦‚è¦ï¼ˆä¸¤å¤§ç›®æ ‡ï¼‰ï¼š

#### âœ… ç›®æ ‡ 1ï¼šå°†â€œåˆ†æ®µæ€»ç»“â€ä¿®æ”¹ä¸ºâ€œå…¨æ–‡æ€»ç»“â€

* åˆ é™¤ `chunk_text` å’Œæ®µè½å¾ªç¯ï¼Œç›´æ¥å¯¹å…¨æ–‡æ„é€  promptã€‚

#### âœ… ç›®æ ‡ 2ï¼šæ”¯æŒå¤šæ–‡æ¡£å¤„ç†æ—¶é¿å…æ˜¾å­˜çˆ†ç‚¸

* æ·»åŠ è‡ªåŠ¨æ£€æµ‹æ˜¾å­˜æ˜¯å¦çˆ†ç‚¸ã€‚
* å¦‚æœå¤±è´¥ï¼ˆçˆ†æ˜¾å­˜ï¼‰ï¼Œå°±åˆ‡æ¢ä¸ºä¸²è¡Œæ–¹å¼å¤„ç†ã€‚

---

## âœ… ä¿®æ”¹åä»£ç ï¼ˆå…³é”®æ–‡ä»¶ `analyze_docs.py` å’Œ `run.py`ï¼‰

---

### ğŸ”§ `app/analyze_docs.py`ï¼ˆé‡æ„ä¸ºå…¨æ–‡æ€»ç»“ï¼Œå¼‚å¸¸å¤„ç†ï¼‰

```python
from transformers import AutoTokenizer, AutoModel
import torch
import re

# åˆå§‹åŒ–æ¨¡å‹
def init_model(gpu_id=0):
    device = f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True) \
        .half().to(device).eval()
    return tokenizer, model, device

# å•ä¸ªæ–‡æ¡£çš„å¤„ç†å‡½æ•°ï¼ˆå…¨æ–‡æ€»ç»“ï¼‰
def summarize_and_tag_single(args):
    fname, text, gpu_id = args
    tokenizer, model, device = init_model(gpu_id)
    try:
        prompt = (
            "è¯·ä½ ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œå¯¹ä»¥ä¸‹æ–‡æ¡£è¿›è¡Œæ‘˜è¦å’Œæ ‡ç­¾æå–ã€‚\n"
            "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š\n"
            "ã€æ€»ç»“ã€‘è¿™é‡Œæ˜¯æ•´ç¯‡æ–‡æ¡£çš„æ€»ç»“å†…å®¹\n"
            "ã€æ ‡ç­¾ã€‘æ ‡ç­¾1ã€æ ‡ç­¾2ã€æ ‡ç­¾3\n\n"
            f"æ–‡æ¡£å†…å®¹ï¼š\n{text}"
        )
        response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=2048)
        return fname, response
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"[è­¦å‘Š] GPU {gpu_id} æ˜¾å­˜ä¸è¶³ï¼š{fname} æ”¹ä¸º CPU ä¸²è¡Œå¤„ç†")
        raise e  # ç”±å¤–å±‚æ•è·
    except Exception as e:
        print(f"[é”™è¯¯] å¤„ç†æ–‡æ¡£ {fname} æ—¶å¼‚å¸¸ï¼š{str(e)}")
        return fname, ""

# è§£ææ¨¡å‹è¾“å‡º
def parse_summary_and_labels(raw_text):
    summary_match = re.search(r"ã€æ€»ç»“ã€‘(.*?)ã€æ ‡ç­¾ã€‘", raw_text, re.S)
    tags_match = re.search(r"ã€æ ‡ç­¾ã€‘(.*)", raw_text, re.S)

    summary = summary_match.group(1).strip() if summary_match else ""
    tag_line = tags_match.group(1).strip() if tags_match else ""
    tags = [t.strip() for t in re.split(r"[ã€,ï¼Œ\s]+", tag_line) if t.strip()]
    return summary, list(set(tags))
```

---

### ğŸ”§ `run.py`ï¼ˆåŠ å…¥è‡ªåŠ¨ä¸²è¡Œé™çº§é€»è¾‘ï¼‰

```python
from app.extract_text import load_all_pdfs
from app.analyze_docs import summarize_and_tag_single, parse_summary_and_labels
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import torch

def safe_process(args):
    try:
        return summarize_and_tag_single(args)
    except RuntimeError as e:
        return None  # æ ‡è®°ä¸ºå¤±è´¥

def main():
    pdf_dir = "data/pdfs"
    docs = load_all_pdfs(pdf_dir)

    gpu_count = max(torch.cuda.device_count(), 1)
    tasks = [(name, text, idx % gpu_count) for idx, (name, text) in enumerate(docs.items())]

    doc_infos = {}

    # ä¼˜å…ˆå°è¯•å¹¶è¡Œå¤„ç†
    print(f"å°è¯•å¹¶è¡Œå¤„ç†æ–‡æ¡£ï¼Œä½¿ç”¨ {gpu_count} ä¸ª GPU ...")
    with ProcessPoolExecutor(max_workers=gpu_count) as executor:
        futures = {executor.submit(safe_process, task): task[0] for task in tasks}
        for future in tqdm(as_completed(futures), total=len(futures), desc="åˆ†ææ–‡æ¡£"):
            result = future.result()
            if result is None:
                continue  # ç¨åä¸²è¡Œé‡è¯•
            fname, raw = result
            summary, tags = parse_summary_and_labels(raw)
            doc_infos[fname] = {"summary": summary, "tags": tags}

    # å¯¹å¤±è´¥çš„æ–‡æ¡£è¿›è¡Œä¸²è¡Œå¤„ç†ï¼ˆé¿å…çˆ†æ˜¾å­˜ï¼‰
    failed = [task for task in tasks if task[0] not in doc_infos]
    if failed:
        print(f"\nä»¥ä¸‹æ–‡æ¡£å°†æ”¹ä¸ºä¸²è¡Œå¤„ç†ï¼ˆå…± {len(failed)} ä¸ªï¼‰ï¼š")
        for fname, _, _ in failed:
            print(f"  - {fname}")
        for fname, text, _ in tqdm(failed, desc="ä¸²è¡Œå¤„ç†"):
            try:
                raw = summarize_and_tag_single((fname, text, 0))[1]
                summary, tags = parse_summary_and_labels(raw)
                doc_infos[fname] = {"summary": summary, "tags": tags}
            except Exception as e:
                print(f"[å¤±è´¥] æ–‡æ¡£ {fname} æ— æ³•å¤„ç†ï¼š{str(e)}")

    # æ„å»ºå›¾è°± + å¯¼å‡º
    build_doc_graph(doc_infos)
    export_to_dify_format(doc_infos)

if __name__ == "__main__":
    main()
```

---

## âœ… æ•´ä½“ä¼˜åŠ¿

| ç‰¹æ€§          | å®ç°                |
| ----------- | ----------------- |
| å…¨æ–‡æ€»ç»“        | æ›¿æ¢æ®µè½åˆ†å‰²ä¸ºå…¨æ–‡ prompt  |
| GPU ä¼˜å…ˆå¹¶è¡Œå¤„ç†  | åˆ©ç”¨å¤š GPU è½®è¯¢è°ƒåº¦      |
| è‡ªåŠ¨é˜²çˆ†æ˜¾å­˜      | å¤±è´¥ååˆ‡æ¢ä¸²è¡Œ + CPU     |
| å¼º Prompt è¾“å‡º | æ¨¡å‹ä¸¥æ ¼è¾“å‡ºã€æ€»ç»“ã€‘å’Œã€æ ‡ç­¾ã€‘æ ¼å¼ |
| è§£æå¥å£®        | å¤šç§åˆ†éš”ç¬¦æ”¯æŒï¼Œé¿å…æå–å¤±è´¥    |

---

æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ æ‰“åŒ… zip æˆ–å†™å¥½å®Œæ•´ `requirements.txt` + å¯åŠ¨è¯´æ˜ï¼Ÿ

```
# app/analyze_docs.py
from transformers import AutoTokenizer, AutoModel
import torch
import gc
import os

model_cache = {}
def init_model(gpu_id=0):
    if gpu_id in model_cache:
        return model_cache[gpu_id]

    device = torch.device(f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu")
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = model.half().to(device).eval()
    model_cache[gpu_id] = (tokenizer, model, device)
    return tokenizer, model, device

def safe_clear_gpu():
    torch.cuda.empty_cache()
    gc.collect()

def summarize_and_tag_full(text, gpu_id=0):
    tokenizer, model, device = init_model(gpu_id)
    try:
        prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æ¡£å†…å®¹å¹¶æå–3-5ä¸ªæ ‡ç­¾ï¼Œè¾“å‡ºæ ¼å¼ï¼šã€æ€»ç»“ã€‘xxxã€æ ‡ç­¾ã€‘xxxï¼š\n{text[:6000]}"
        response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=512)
        return response
    except RuntimeError as e:
        print(f"[Error] GPU {gpu_id} failed: {e}")
        return "ã€æ€»ç»“ã€‘å¤±è´¥ã€æ ‡ç­¾ã€‘"
    finally:
        safe_clear_gpu()

def parse_summary_and_labels(text):
    import re
    summary_match = re.search(r"ã€æ€»ç»“ã€‘(.*?)ã€æ ‡ç­¾ã€‘", text, re.S)
    tags_match = re.findall(r"ã€æ ‡ç­¾ã€‘(.*?)\n?", text, re.S)

    summary = summary_match.group(1).strip() if summary_match else text
    tags = []
    for tag_line in tags_match:
        tags += [t.strip("ï¼š:ï¼Œ, ") for t in tag_line.split("ã€") if t.strip()]
    return summary.strip(), list(set(tags))

# app/extract_text.py
import fitz, os

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join(page.get_text().strip() for page in doc)

def load_all_pdfs(folder):
    return {
        fn: extract_text_from_pdf(os.path.join(folder, fn))
        for fn in os.listdir(folder) if fn.lower().endswith(".pdf")
    }

# app/build_graph.py
import networkx as nx
from pyvis.network import Network
from sentence_transformers import SentenceTransformer, util
import torch, os

embed_model = SentenceTransformer(
    "paraphrase-multilingual-MiniLM-L12-v2",
    device="cuda" if torch.cuda.is_available() else "cpu"
)

def build_doc_graph(doc_infos, sim_threshold=0.65, output_path="output/graph.html"):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    names = list(doc_infos.keys())
    summaries = [doc_infos[n]["summary"] for n in names]

    embeddings = embed_model.encode(summaries, convert_to_tensor=True)

    G = nx.Graph()
    for n in names:
        G.add_node(n, label=n, title=doc_infos[n]["summary"])

    for i in range(len(names)):
        for j in range(i+1, len(names)):
            n1, n2 = names[i], names[j]
            tags1, tags2 = set(doc_infos[n1]["tags"]), set(doc_infos[n2]["tags"])
            common = tags1 & tags2
            score = util.cos_sim(embeddings[i], embeddings[j]).item()
            if common:
                G.add_edge(n1, n2, label="æ ‡ç­¾ï¼š" + "ã€".join(common))
            elif score >= sim_threshold:
                G.add_edge(n1, n2, label=f"ç›¸ä¼¼({score:.2f})")

    net = Network(height="800px", width="100%", directed=False, notebook=False)
    net.from_nx(G)
    net.show_buttons(filter_=['physics'])
    net.show(output_path)
    print("çŸ¥è¯†å›¾è°±å·²ç”Ÿæˆï¼š", output_path)

# app/export_dify.py
import json, os

def export_to_dify_format(doc_infos, output_file="output/dify_dataset.json"):
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    arr = []
    for name, info in doc_infos.items():
        arr.append({
            "id": name,
            "content": info["summary"],
            "metadata": {"tags": info["tags"]}
        })
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(arr, f, ensure_ascii=False, indent=2)
    print("Dify æ•°æ®å·²å¯¼å‡ºï¼š", output_file)

# run.py
from app.extract_text import load_all_pdfs
from app.analyze_docs import summarize_and_tag_full, parse_summary_and_labels
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
from concurrent.futures import ProcessPoolExecutor
import torch
from tqdm import tqdm

def process_doc(args):
    name, text, gpu = args
    raw = summarize_and_tag_full(text, gpu)
    summary, tags = parse_summary_and_labels(raw)
    print(f"{name} â†’ æ ‡ç­¾ï¼š{tags}")
    return name, {"summary": summary, "tags": tags}

if __name__ == "__main__":
    pdfs = load_all_pdfs("data/pdfs")
    gpu_count = max(torch.cuda.device_count(), 1)
    tasks = [(n, t, idx % gpu_count) for idx, (n, t) in enumerate(pdfs.items())]

    doc_infos = {}
    with ProcessPoolExecutor(max_workers=gpu_count) as exe:
        for name, info in tqdm(exe.map(process_doc, tasks), total=len(tasks), desc="åˆ†ææ–‡æ¡£"):
            doc_infos[name] = info

    build_doc_graph(doc_infos)
    export_to_dify_format(doc_infos)

```
ä½ æåˆ°çš„åŠŸèƒ½éœ€æ±‚ï¼ˆ**å…¨æ–‡æ‘˜è¦** + **è¯­ä¹‰è¿çº¿æ”¹è¿›**ï¼‰æˆ‘å·²ç»å®Œæ•´ç†è§£ï¼Œå¹¶æ ¹æ®ä½ æä¾›çš„åŸå§‹ä»£ç åšå‡ºä»¥ä¸‹**æ”¹åŠ¨ä¸è¡¥å…¨**ï¼ŒåŒ…æ‹¬ï¼š

---

## âœ… ä¿®æ”¹/æ–°å¢ç‚¹æ€»ç»“ï¼š

### 1. `analyze_docs.py`ï¼š

* âœ… æ”¹ä¸º `summarize_and_tag_full()`ï¼ˆæ•´ç¯‡æ‘˜è¦+æ ‡ç­¾ï¼‰ã€‚
* âœ… ä¿ç•™æ¨¡å‹ç¼“å­˜ä¸ GPU æ§åˆ¶èƒ½åŠ›ã€‚

### 2. `run.py`ï¼š

* âœ… ä¸»å‡½æ•°ä½¿ç”¨ `summarize_and_tag_full()` å–ä»£åŸæ¥çš„ `summarize_and_tag_single()`ã€‚

---

## âœ… ä¿®æ”¹åä»£ç å¦‚ä¸‹ï¼š

---

### âœ… `app/analyze_docs.py`

```python
# app/analyze_docs.py
from transformers import AutoTokenizer, AutoModel
import torch
import gc
import os
import re

model_cache = {}

def init_model(gpu_id=0):
    if gpu_id in model_cache:
        return model_cache[gpu_id]

    device = torch.device(f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu")
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = model.half().to(device).eval()
    model_cache[gpu_id] = (tokenizer, model, device)
    return tokenizer, model, device

def safe_clear_gpu():
    torch.cuda.empty_cache()
    gc.collect()

def summarize_and_tag_full(text, gpu_id=0):
    tokenizer, model, device = init_model(gpu_id)

    try:
        # æ§åˆ¶é•¿åº¦ï¼ŒChatGLM3 å»ºè®® <= 6000 æ±‰å­—
        input_text = text[:6000]
        prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æ¡£å†…å®¹å¹¶æå–3-5ä¸ªæ ‡ç­¾ï¼Œè¾“å‡ºæ ¼å¼ï¼šã€æ€»ç»“ã€‘xxxã€æ ‡ç­¾ã€‘xxxï¼š\n{input_text}"
        response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=512)
        return response
    except RuntimeError as e:
        print(f"[Error] GPU {gpu_id} failed: {e}")
        return "ã€æ€»ç»“ã€‘å¤±è´¥ã€æ ‡ç­¾ã€‘"
    finally:
        safe_clear_gpu()

def parse_summary_and_labels(text):
    summary_match = re.search(r"ã€æ€»ç»“ã€‘(.*?)ã€æ ‡ç­¾ã€‘", text, re.S)
    tags_match = re.findall(r"ã€æ ‡ç­¾ã€‘(.*?)\n?", text, re.S)

    summary = summary_match.group(1).strip() if summary_match else text
    tags = []
    for tag_line in tags_match:
        tags += [t.strip("ï¼š:ï¼Œ, ") for t in tag_line.split("ã€") if t.strip()]
    return summary.strip(), list(set(tags))
```

---

### âœ… `run.py`

```python
# run.py
from app.extract_text import load_all_pdfs
from app.analyze_docs import summarize_and_tag_full, parse_summary_and_labels
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
from concurrent.futures import ProcessPoolExecutor
import torch
from tqdm import tqdm

def process_doc(args):
    name, text, gpu = args
    raw = summarize_and_tag_full(text, gpu)
    summary, tags = parse_summary_and_labels(raw)
    print(f"{name} â†’ æ ‡ç­¾ï¼š{tags}")
    return name, {"summary": summary, "tags": tags}

if __name__ == "__main__":
    pdfs = load_all_pdfs("data/pdfs")
    gpu_count = max(torch.cuda.device_count(), 1)
    tasks = [(n, t, idx % gpu_count) for idx, (n, t) in enumerate(pdfs.items())]

    doc_infos = {}
    with ProcessPoolExecutor(max_workers=gpu_count) as exe:
        for name, info in tqdm(exe.map(process_doc, tasks), total=len(tasks), desc="åˆ†ææ–‡æ¡£"):
            doc_infos[name] = info

    build_doc_graph(doc_infos)
    export_to_dify_format(doc_infos)
```

---

> å…¶ä»–æ–‡ä»¶å¦‚ `build_graph.py`, `extract_text.py`, `export_dify.py` å·²ä¸ä½ çš„è¦æ±‚ä¸€è‡´ï¼Œæ— éœ€æ”¹åŠ¨ã€‚

---

## âœ… ä½¿ç”¨è¯´æ˜ï¼ˆæ›´æ–°ï¼‰ï¼š

* ğŸ“„ **å…¨æ–‡æ‘˜è¦**ï¼š`summarize_and_tag_full()` ä»…è°ƒç”¨ä¸€æ¬¡æ¨¡å‹ï¼Œé¿å…åˆ†æ®µæ‘˜è¦ã€‚
* ğŸ§  **è¯­ä¹‰ç›¸ä¼¼åº¦è¿è¾¹**ï¼šä½¿ç”¨ `sentence-transformers` è‡ªåŠ¨åŠ â€œç›¸ä¼¼â€è¾¹ã€‚
* ğŸ¯ **æ ‡ç­¾ä¼˜å…ˆ**ï¼šè‹¥ä¸¤ä¸ªæ–‡æ¡£æœ‰æ ‡ç­¾äº¤é›†ï¼Œåˆ™ä½¿ç”¨æ ‡ç­¾è¿è¾¹ï¼Œå¦åˆ™å†åˆ¤æ–­ç›¸ä¼¼åº¦ã€‚
* âš™ï¸ **å¤š GPU æ”¯æŒ**ï¼šè‡ªåŠ¨è½®è¯¢ GPU åˆ†é…ä»»åŠ¡ã€‚
* ğŸŒ **å›¾è°±å¯è§†åŒ–**ï¼šç”Ÿæˆ `output/graph.html`ï¼Œäº¤äº’å¼æŸ¥çœ‹ã€‚
* ğŸ§¾ **Dify å¯¼å‡º**ï¼šè¾“å‡ºç»“æ„åŒ– JSON æ–‡ä»¶ï¼Œé€‚é… Dify ä¸Šä¼ ã€‚

---

å¦‚ä½ è¿˜éœ€è¦ç”Ÿæˆ**å¯éƒ¨ç½²çš„é¡¹ç›®ç›®å½•ç»“æ„**æˆ–**æ‰“åŒ…ä¸º Docker é•œåƒ/ç½‘é¡µäº¤äº’å‰ç«¯**ï¼Œä¹Ÿå¯ä»¥ç»§ç»­å‘Šè¯‰æˆ‘ã€‚

```
C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\cuda\IndexKernel.cu:93: block: [1086,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && "index out of bounds"` failed.

torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 44.67 GiB of which 100.25 MiB is free. Of the allocated memory 9.21 GiB is allocated by PyTorch, and 12.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
```
ä¸‹é¢çš„å®Œæ•´ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åšäº†ä¸¤ç‚¹æ ¸å¿ƒæ”¹è¿›ï¼š

1. **å…¨æ–‡æ‘˜è¦**ï¼šå°†æ¨¡å‹è°ƒç”¨æ”¹ä¸ºå¯¹æ•´ç¯‡æ–‡æ¡£åšä¸€æ¬¡æ‘˜è¦ï¼Œè€Œä¸æ˜¯åˆ†æ®µæ‘˜è¦ã€‚
2. **è‡ªåŠ¨è¿çº¿**ï¼šåœ¨å›¾è°±æ„å»ºæ—¶ï¼Œä¸ä»…ä¾èµ–æ ‡ç­¾äº¤é›†ï¼Œæ›´åŠ å…¥è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆ`sentence-transformers`ï¼‰ï¼Œå½“ä¸¤ç¯‡æ–‡æ¡£ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼æ—¶è‡ªåŠ¨æ·»åŠ â€œç›¸ä¼¼â€è¾¹ã€‚

---

## 1. `app/analyze_docs.py`ï¼ˆå¯¹å…¨æ–‡æ‘˜è¦ + æ ‡ç­¾ï¼‰

```python
# app/analyze_docs.py
# analyze_docs.py
from transformers import AutoTokenizer, AutoModel
import torch
import gc
import os

# æ”¯æŒåˆ†é… GPU çš„åˆå§‹åŒ–
model_cache = {}
def init_model(gpu_id=0):
    if gpu_id in model_cache:
        return model_cache[gpu_id]

    device = torch.device(f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu")
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = model.half().to(device).eval()
    model_cache[gpu_id] = (tokenizer, model, device)
    return tokenizer, model, device

def safe_clear_gpu():
    torch.cuda.empty_cache()
    gc.collect()

def summarize_and_tag_single(args):
    filename, text, gpu_id = args
    tokenizer, model, device = init_model(gpu_id)

    try:
        # é˜²æ­¢ index è¶Šç•Œå¼‚å¸¸ï¼šåˆ‡ç‰‡é•¿åº¦ä¸è¶…è¿‡æ¨¡å‹é™åˆ¶ï¼ˆChatGLM3 ä¸€èˆ¬ä¸è¶…è¿‡ 8192 tokenï¼‰
        prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æ¡£å†…å®¹å¹¶æå–3-5ä¸ªæ ‡ç­¾ï¼Œè¾“å‡ºæ ¼å¼ï¼šã€æ€»ç»“ã€‘xxxã€æ ‡ç­¾ã€‘xxxï¼š\n{text[:6000]}"
        response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=512)
        return filename, response
    except RuntimeError as e:
        print(f"[Error] {filename} GPU {gpu_id} failed: {e}")
        return filename, "ã€æ€»ç»“ã€‘å¤±è´¥ã€æ ‡ç­¾ã€‘"
    finally:
        safe_clear_gpu()

def parse_summary_and_labels(text):
    import re
    summary_match = re.search(r"ã€æ€»ç»“ã€‘(.*?)ã€æ ‡ç­¾ã€‘", text, re.S)
    tags_match = re.findall(r"ã€æ ‡ç­¾ã€‘(.*?)\n?", text, re.S)

    summary = summary_match.group(1).strip() if summary_match else text
    tags = []
    for tag_line in tags_match:
        tags += [t.strip("ï¼š:ï¼Œ, ") for t in tag_line.split("ã€") if t.strip()]
    return summary.strip(), list(set(tags))

```

---

## 2. `app/extract_text.py`ï¼ˆæ— éœ€æ”¹åŠ¨ï¼‰

```python
import fitz, os

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join(page.get_text().strip() for page in doc)

def load_all_pdfs(folder):
    return {
        fn: extract_text_from_pdf(os.path.join(folder, fn))
        for fn in os.listdir(folder) if fn.lower().endswith(".pdf")
    }
```

---

## 3. `app/build_graph.py`ï¼ˆæ ‡ç­¾ï¼‹è¯­ä¹‰ç›¸ä¼¼åº¦è¿çº¿ï¼‰

```python
# app/build_graph.py
import networkx as nx
from pyvis.network import Network
from sentence_transformers import SentenceTransformer, util
import torch, os

# è½»é‡å‘é‡æ¨¡å‹
embed_model = SentenceTransformer(
    "paraphrase-multilingual-MiniLM-L12-v2",
    device="cuda" if torch.cuda.is_available() else "cpu"
)

def build_doc_graph(doc_infos, sim_threshold=0.65, output_path="output/graph.html"):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    names = list(doc_infos.keys())
    summaries = [doc_infos[n]["summary"] for n in names]

    # è®¡ç®— embeddings
    embeddings = embed_model.encode(summaries, convert_to_tensor=True)

    G = nx.Graph()
    for n in names:
        G.add_node(n, label=n, title=doc_infos[n]["summary"])

    # ç”¨æ ‡ç­¾äº¤é›†å’Œè¯­ä¹‰ç›¸ä¼¼åº¦åŒé‡è¿è¾¹
    for i in range(len(names)):
        for j in range(i+1, len(names)):
            n1, n2 = names[i], names[j]
            tags1, tags2 = set(doc_infos[n1]["tags"]), set(doc_infos[n2]["tags"])
            common = tags1 & tags2
            score = util.cos_sim(embeddings[i], embeddings[j]).item()
            if common:
                G.add_edge(n1, n2, label="æ ‡ç­¾ï¼š"+ "ã€".join(common))
            elif score >= sim_threshold:
                G.add_edge(n1, n2, label=f"ç›¸ä¼¼({score:.2f})")

    net = Network(height="800px", width="100%", directed=False, notebook=False)
    net.from_nx(G)
    net.show_buttons(filter_=['physics'])
    net.show(output_path)
    print("çŸ¥è¯†å›¾è°±å·²ç”Ÿæˆï¼š", output_path)
```

---

## 4. `app/export_dify.py`

```python
import json, os

def export_to_dify_format(doc_infos, output_file="output/dify_dataset.json"):
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    arr = []
    for name, info in doc_infos.items():
        arr.append({
            "id": name,
            "content": info["summary"],
            "metadata": {"tags": info["tags"]}
        })
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(arr, f, ensure_ascii=False, indent=2)
    print("Dify æ•°æ®å·²å¯¼å‡ºï¼š", output_file)
```

---

## 5. `run.py`ï¼ˆä¸»æµç¨‹å¹¶è¡Œ & å¤š GPUï¼‰

```python
# run.py
from app.extract_text import load_all_pdfs
from app.analyze_docs import summarize_and_tag_full, parse_summary_and_labels
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
from concurrent.futures import ProcessPoolExecutor
import torch
from tqdm import tqdm

def process_doc(args):
    name, text, gpu = args
    raw = summarize_and_tag_full(text, gpu)
    summary, tags = parse_summary_and_labels(raw)
    print(f"{name} â†’ æ ‡ç­¾ï¼š{tags}")
    return name, {"summary": summary, "tags": tags}

if __name__ == "__main__":
    pdfs = load_all_pdfs("data/pdfs")
    gpu_count = max(torch.cuda.device_count(), 1)
    tasks = [(n, t, idx % gpu_count) for idx, (n, t) in enumerate(pdfs.items())]

    doc_infos = {}
    with ProcessPoolExecutor(max_workers=gpu_count) as exe:
        for name, info in tqdm(exe.map(process_doc, tasks), total=len(tasks), desc="åˆ†ææ–‡æ¡£"):
            doc_infos[name] = info

    build_doc_graph(doc_infos)
    export_to_dify_format(doc_infos)
```

---

### ä½¿ç”¨è¯´æ˜

1. **å…¨æ–‡æ‘˜è¦**ï¼š`summarize_and_tag_full` å¯¹æ•´ç¯‡æ–‡æ¡£åšä¸€æ¬¡è°ƒç”¨ï¼›
2. **åŒé‡è¿è¾¹**ï¼šæ ‡ç­¾äº¤é›†ä¼˜å…ˆï¼Œè‹¥æ— äº¤é›†ä¸”ç›¸ä¼¼åº¦ â‰¥ 0.65ï¼Œåˆ™è¿â€œç›¸ä¼¼â€è¾¹ï¼›
3. **ç•Œé¢å±•ç¤º**ï¼šç”Ÿæˆçš„ `output/graph.html` æ˜¯äº¤äº’å¼ç½‘é¡µï¼›
4. **å¤š GPU**ï¼šè‡ªåŠ¨è½®è¯¢åˆ†é…æ˜¾å¡ã€‚

è¿™æ ·å°±èƒ½ç¡®ä¿ **æ¯ç¯‡æ–‡æ¡£åªåšä¸€æ¬¡å…¨æ–‡æ‘˜è¦**ï¼Œå¹¶ä¸” **å›¾ä¸­å‡ºç°åŸºäºæ ‡ç­¾æˆ–è¯­ä¹‰çš„è¿çº¿**ã€‚

```
[
  {
    "id": "p-AIæŠ€æœ¯å…¥é—¨åˆ†äº«-äº”ä¸åŒæ¨¡å¼æ™ºèƒ½ä½“ 0316.pdf",
    "summary": "æœ¬æ–‡æ¡£ä»‹ç»äº†AIæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„å››ç§å…³é”®æ¨¡å¼ï¼šé“¾å¼å·¥ä½œæµã€å¹¶è¡ŒåŒ–å·¥ä½œæµã€è·¯ç”±å·¥ä½œæµå’Œç¼–æ’å™¨-å·¥ä½œè€…æ¨¡å¼ã€‚æ¯ç§æ¨¡å¼éƒ½æœ‰å…¶é€‚ç”¨åœºæ™¯å’Œä¼˜ç‚¹ï¼Œå¯ä»¥æ ¹æ®ä»»åŠ¡çš„ç‰¹ç‚¹å’Œéœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥ä½œæ¨¡å¼ä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚",
    "tags": ""
  },
  {
    "id": "p-ä¾›åº”åˆ¶é€ è‡ªæœ‰çŸ¥è¯†åº“ï¼ˆDeepSeek R1ï¼‰æ¨¡å‹éƒ¨ç½²åŠå·¥ç¨‹æ•°å­—äººæ–¹æ¡ˆ0225-2.pdf",
    "summary": "æœ¬æ–‡ä¸»è¦ä»‹ç»äº†Security Levelä¸ºconfidentialçš„ä¾›åº”åˆ¶é€ è‡ªæœ‰çŸ¥è¯†åº“ï¼ˆDeepSeek R1ï¼‰æ¨¡å‹éƒ¨ç½²åŠå·¥ç¨‹æ•°å­—äººæ–¹æ¡ˆï¼Œå¹¶ç»“åˆæ™ºèƒ½åˆ¶é€ æœªæ¥è§„åˆ’ï¼Œæ‰“é€ æ™ºèƒ½åˆ¶é€ é¢†åŸŸâ€œå°æˆ˜é¢…â€å’Œâ€œå°åƒæ‰‹è§‚éŸ³â€ç³»ç»Ÿã€‚ä¸»è¦å†…å®¹åŒ…æ‹¬AIåœ¨å†›äº‹ç³»ç»Ÿä¸­çš„åº”ç”¨æˆæœï¼Œç»“åˆæ™ºèƒ½åˆ¶é€ æœªæ¥è§„åˆ’ï¼Œä»¥åŠâ€œå°æˆ˜é¢…â€ç³»ç»Ÿçš„çŸ¥è¯†ä½“ç³»æ¶æ„ã€‚",
    "tags": ""
  },
  {
    "id": "p-åˆ¶é€ å·¥è‰ºæ¦‚è¿°.pdf",
    "summary": "è¿™æ˜¯ä¸€ä»½å…³äºäº§å“åˆ¶é€ å·¥è‰ºçš„æ¦‚è¿°ï¼Œå†…å®¹åŒ…æ‹¬å•æ¿åˆ¶é€ å·¥è‰ºã€æ•´æœºåˆ¶é€ å·¥è‰ºå’ŒSMTåˆ¶é€ å·¥è‰ºã€‚é€šè¿‡å¯¹å„å·¥åºå·¥è‰ºè¿‡ç¨‹å’Œå…³é”®è´¨é‡é£é™©ç®¡æ§è¦ç‚¹çš„ä»‹ç»ï¼Œæå‡äº†å…¨æµç¨‹è´¨é‡æ„è¯†ã€‚ä¸»è¦æ ‡ç­¾ä¸ºï¼šåˆ¶é€ å·¥è‰ºã€äº§å“åˆ¶é€ ã€SMTå·¥è‰ºã€PCBåˆ¶é€ ã€‚\n\nç¬¬2æ®µï¼šã€æ€»ç»“ã€‘è¯¥æ–‡æ¡£ä¸»è¦ä»‹ç»äº†å•æ¿åˆ¶é€ å·¥è‰ºä¸­çš„SMTç„Šæ¥/å‹æ¥åˆ¶é€ å·¥è‰ºã€æ•´æœºåˆ¶é€ å·¥è‰ºä»¥åŠSMTåˆ¶é€ å·¥è‰ºçš„ç›¸å…³å†…å®¹ï¼ŒåŒ…æ‹¬ç„Šæ¥/å‹æ¥åˆ¶é€ å·¥è‰ºã€å•æ¿è£…é…åˆ¶é€ å·¥è‰ºã€æµ‹è¯•/ç»´ä¿®ç­‰ç¯èŠ‚ã€‚å…¶ä¸­ï¼Œåˆè¯¦ç»†ä»‹ç»äº†å„ç§åˆ¶é€ å·¥è‰ºçš„è¿‡ç¨‹ã€åŸç†ã€è®¾å¤‡åŠå…³é”®è´¨é‡æ§åˆ¶è¦ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜æåˆ°äº†å›æµç„Šå·¥è‰ºã€AOIè‡ªåŠ¨å…‰å­¦æ£€æµ‹ã€X-Rayè®¾å¤‡åŠæ£€æµ‹åŸç†ç­‰ç›¸å…³å†…å®¹ã€‚",
    "tags": ""
  }
]


Both `max_new_tokens` (=512) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "C:\Users\Administrator\.cache\huggingface\modules\transformers_modules\THUDM\chatglm3-6b\e9e0406d062cdb887444fe5bd546833920abd4ac\modeling_chatglm.py", line 182, in apply_rotary_pos_emb
    )
    x_out2 = x_out2.flatten(3)
    return torch.cat((x_out2, x_pass), dim=-1)
           ~~~~~~~~~ <--- HERE
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 44.67 GiB of which 18.94 MiB is free. Of the allocated memory 12.13 GiB is allocated by PyTorch, and 27.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)   


RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.
```
ä»¥ä¸‹æ˜¯å®Œæ•´æ”¯æŒ **å¤š GPU å¹¶è¡Œ** çš„ä»£ç ç‰ˆæœ¬ã€‚æˆ‘ä»¬ä¼šå¯¹ `analyze_docs.py` å’Œ `run.py` ä¸¤ä¸ªæ¨¡å—åšä¿®æ”¹ï¼Œå¹¶ä¿è¯æ¯ä¸ªè¿›ç¨‹è‡ªåŠ¨åˆ†é…ä¸åŒ GPUï¼Œä»¥é¿å…é‡å¤å ç”¨åŒä¸€å—æ˜¾å¡å†…å­˜ã€‚

---

## ğŸ“ ç›®å½•ç»“æ„ï¼ˆä¿æŒä¸€è‡´ï¼‰ï¼š

```
your_project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ extract_text.py
â”‚   â”œâ”€â”€ analyze_docs.py       â† âœ… ä¿®æ”¹
â”‚   â”œâ”€â”€ build_graph.py
â”‚   â”œâ”€â”€ export_dify.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/                 â† å­˜æ”¾ PDF æ–‡æ¡£
â”œâ”€â”€ run.py                    â† âœ… ä¿®æ”¹
```

---

## âœ… `app/analyze_docs.py`ï¼ˆæ”¯æŒ GPU ID è‡ªåŠ¨åˆ†é…ï¼‰

```python
# app/analyze_docs.py
from transformers import AutoTokenizer, AutoModel
import torch

def init_model(gpu_id=0):
    device = f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).half().to(device).eval()
    return tokenizer, model, device

def chunk_text(text, max_len=1500):
    return [text[i:i+max_len] for i in range(0, len(text), max_len)]

def summarize_and_tag_single(args):
    filename, text, gpu_id = args
    tokenizer, model, device = init_model(gpu_id)
    chunks = chunk_text(text)
    combined_summary = ""
    for i, chunk in enumerate(chunks):
        prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æ¡£å†…å®¹å¹¶æå–3-5ä¸ªæ ‡ç­¾ï¼Œè¾“å‡ºæ ¼å¼ï¼šã€æ€»ç»“ã€‘xxxã€æ ‡ç­¾ã€‘xxxï¼š\n{chunk}"
        response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=512)
        combined_summary += f"\nç¬¬{i+1}æ®µï¼š{response}\n"
    return filename, combined_summary

def parse_summary_and_labels(text):
    import re
    summary_match = re.search(r"ã€æ€»ç»“ã€‘(.*?)ã€æ ‡ç­¾ã€‘", text, re.S)
    tags_match = re.findall(r"ã€æ ‡ç­¾ã€‘(.*?)\n?", text, re.S)

    summary = summary_match.group(1).strip() if summary_match else text
    tags = []
    for tag_line in tags_match:
        tags += [t.strip("ï¼š:ï¼Œ, ") for t in tag_line.split("ã€") if t.strip()]
    return summary.strip(), list(set(tags))
```

---

## âœ… `run.py`ï¼ˆè‡ªåŠ¨åˆ†é… GPU ç»™ä¸åŒè¿›ç¨‹ï¼‰

```python
# run.py
# run.py
import os
from app.extract_text import extract_text_from_pdf
from app.analyze_docs import summarize_and_tag_single, parse_summary_and_labels
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
from concurrent.futures import ProcessPoolExecutor
import torch
from tqdm import tqdm

def main():
    pdf_dir = "data/pdfs"
    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(".pdf")]

    # Step 1: æå–æ–‡æœ¬
    pdf_texts = {}
    for file in tqdm(pdf_files, desc="æå–PDFæ–‡æœ¬"):
        text = extract_text_from_pdf(os.path.join(pdf_dir, file))
        pdf_texts[file] = text

    # Step 2: å¤šGPUå¹¶è¡Œè°ƒç”¨
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ–¥ï¸ æ£€æµ‹åˆ° {gpu_count} å— GPUï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç†...")

    task_args = []
    for idx, (fname, text) in enumerate(pdf_texts.items()):
        assigned_gpu = idx % gpu_count
        task_args.append((fname, text, assigned_gpu))

    doc_infos = {}
    with ProcessPoolExecutor(max_workers=gpu_count) as executor:
        results = executor.map(summarize_and_tag_single, task_args)
        for fname, result in tqdm(results, total=len(task_args), desc="åˆ†ææ–‡æ¡£"):
            summary, tags = parse_summary_and_labels(result)
            print(f"ğŸ“„ {fname} æ ‡ç­¾ï¼š{tags}")
            doc_infos[fname] = {"summary": summary, "tags": tags}

    # Step 3: æ„å»ºå›¾è°±
    build_doc_graph(doc_infos)

    # Step 4: å¯¼å‡ºä¸º Dify æ ¼å¼
    export_to_dify_format(doc_infos)

if __name__ == "__main__":
    main()

```


---

## âš ï¸ æ³¨æ„äº‹é¡¹

| äº‹é¡¹    | å†…å®¹                                               |
| ----- | ------------------------------------------------ |
| å¤šå¡ç¯å¢ƒ  | é»˜è®¤è½®æµåˆ†é… GPUï¼Œä¾‹å¦‚æ–‡æ¡£ 0 â†’ GPU0ï¼Œæ–‡æ¡£ 1 â†’ GPU1ï¼Œæ–‡æ¡£ 2 â†’ GPU0 |
| å†…å­˜é™åˆ¶  | æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½ä¸€æ¬¡æ¨¡å‹ï¼Œæ¯ä¸ªæ˜¾å¡çº¦å ç”¨ 13GB æ˜¾å­˜ï¼ˆchatglm3-6b åŠç²¾åº¦ï¼‰    |
| å•å¡ä¹Ÿå…¼å®¹ | å¦‚æœåªæœ‰ä¸€å— GPUï¼Œä»ç„¶æ”¯æŒï¼Œåªæ˜¯ä¸ä¼šçœŸæ­£å¹¶è¡Œ                         |

---

## âœ… æ•ˆæœç¤ºä¾‹ï¼ˆè¾“å‡ºï¼‰ï¼š

```bash
ğŸ–¥ï¸ æ£€æµ‹åˆ° 2 å— GPUï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç†...
åˆ†ææ–‡æ¡£: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:38<00:00,  9.61s/it]
ğŸ“„ doc1.pdf æ ‡ç­¾ï¼š['ç®—æ³•', 'æ·±åº¦å­¦ä¹ ']
ğŸ“„ doc2.pdf æ ‡ç­¾ï¼š['ç”µæœºæ§åˆ¶', 'è‡ªåŠ¨åŒ–']
...
```

---

å¦‚éœ€æ”¯æŒæ˜¾å¡æ‰‹åŠ¨æŒ‡å®šï¼Œæˆ–ç»“åˆ `ray` / `deepspeed` / `accelerate` ä¼˜åŒ–æ¨¡å‹åŠ è½½ï¼Œä¹Ÿå¯ä»¥å†è¿›ä¸€æ­¥å‡çº§ã€‚æ˜¯å¦éœ€è¦æˆ‘æä¾› `ray` å¹¶è¡Œæ¡†æ¶çš„ç‰ˆæœ¬ï¼Ÿ
æ˜ç™½äº†ï¼ä½ å·²ç»è§£å†³äº†å¹¶è¡Œå¤šGPUæ‰§è¡Œé—®é¢˜ï¼Œä½†**ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±ä¸­ï¼Œæ–‡æ¡£èŠ‚ç‚¹ä¹‹é—´æ²¡æœ‰â€œè¾¹â€**ï¼ˆå¦‚â€œåŒå±äº§å“â€ã€â€œå†…å®¹ç›¸å…³â€ã€â€œåŒ…å«â€ç­‰å…³ç³»ï¼‰ã€‚

---

## ğŸ§  é—®é¢˜æœ¬è´¨ï¼š

ç›®å‰çš„ `build_doc_graph.py` è„šæœ¬ **åªæ˜¯å°†æ¯ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªç‹¬ç«‹èŠ‚ç‚¹**ï¼Œå¹¶æœªåˆ†ææ–‡æ¡£é—´çš„è¯­ä¹‰å…³ç³»ï¼Œå› æ­¤å›¾ä¸­æ˜¯â€œæ— è¾¹å­¤å²›â€ã€‚

---

## âœ… ç›®æ ‡ï¼š

ä½ å¸Œæœ›è‡ªåŠ¨åˆ¤æ–­ **æ–‡æ¡£ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼ˆå¦‚ä¸»é¢˜ç›¸è¿‘ã€å†…å®¹åŒ…å«ã€æ¥è‡ªåŒä¸€äº§å“çº¿ï¼‰**ï¼Œå¹¶åœ¨å›¾ä¸­æ·»åŠ  **è¾¹ï¼ˆEdgeï¼‰** è¡¨ç¤ºè¿™äº›å…³ç³»ã€‚

---

## âœ… æ”¹è¿›æ€è·¯ï¼š

æˆ‘ä»¬æ–°å¢ä¸€æ®µé€»è¾‘æ¥ï¼š

1. é€šè¿‡ BERT ç­‰æ¨¡å‹æå–æ¯ç¯‡æ–‡æ¡£çš„ embeddingï¼ˆå‘é‡è¡¨ç¤ºï¼‰
2. è®¡ç®— **æ–‡æ¡£é—´ç›¸ä¼¼åº¦**
3. å½“ç›¸ä¼¼åº¦é«˜äºæŸé˜ˆå€¼ï¼ˆå¦‚ 0.7ï¼‰æ—¶æ·»åŠ è¾¹ï¼ˆå…³ç³»åå¯é»˜è®¤æ˜¯â€œå†…å®¹ç›¸å…³â€æˆ–â€œäº§å“ç›¸å…³â€ï¼‰

---

## âœ… æ”¹è¿›åçš„ `build_doc_graph.py`

ä»¥ä¸‹æ˜¯å®Œæ•´å¯è¿è¡Œçš„ç‰ˆæœ¬ï¼ˆå«è¾¹ç”Ÿæˆé€»è¾‘ï¼‰ï¼š

```python
# app/build_graph.py
from pyvis.network import Network
import networkx as nx
from sentence_transformers import SentenceTransformer, util
import torch
import os

def build_doc_graph_web(doc_infos, sim_threshold=0.7, output_path="output/graph.html"):
    os.makedirs("output", exist_ok=True)
    G = nx.Graph()
    doc_names = list(doc_infos.keys())
    summaries = [doc_infos[name]["summary"] for name in doc_names]

    # åŠ è½½è½»é‡æ¨¡å‹ç”¨äºç›¸ä¼¼åº¦
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2", device="cuda" if torch.cuda.is_available() else "cpu")
    embeddings = model.encode(summaries, convert_to_tensor=True)

    # æ·»åŠ èŠ‚ç‚¹
    for name in doc_names:
        G.add_node(name, label=name)

    # æ·»åŠ è¾¹ï¼ˆåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
    for i in range(len(doc_names)):
        for j in range(i + 1, len(doc_names)):
            sim = util.cos_sim(embeddings[i], embeddings[j]).item()
            if sim >= sim_threshold:
                G.add_edge(doc_names[i], doc_names[j], weight=round(sim, 2), label="ç›¸ä¼¼")

    # ç”¨ pyvis å¯è§†åŒ–ä¸ºç½‘é¡µ
    net = Network(height="800px", width="100%", notebook=False, font_color="black", directed=False)
    net.from_nx(G)
    net.show_buttons(filter_=['physics'])  # æ˜¾ç¤ºå¸ƒå±€è°ƒèŠ‚æŒ‰é’®
    net.show(output_path,notbook=Flase)

    print(f"âœ… äº¤äº’å¼çŸ¥è¯†å›¾è°±ç½‘é¡µå·²ç”Ÿæˆï¼š{output_path}")

```

---

## ğŸ§ª ç¤ºä¾‹æ•ˆæœï¼ˆå«è¾¹ï¼‰ï¼š

* **èŠ‚ç‚¹**ï¼šPDF æ–‡æ¡£æ–‡ä»¶å
* **è¾¹**ï¼šç›¸ä¼¼åº¦ > 0.7 æ—¶åŠ â€œç›¸å…³â€è¾¹
* **æ”¯æŒä¸­æ–‡æ‘˜è¦**

---

## âœ… ä¾èµ–è¯´æ˜ï¼ˆæ–°ç”¨åˆ°ï¼‰ï¼š

```bash
pip install sentence-transformers
```

---

## ğŸš€ å¯é€‰å¢å¼ºåŠŸèƒ½ï¼š

| åŠŸèƒ½       | æè¿°                                  |
| -------- | ----------------------------------- |
| è‡ªå®šä¹‰å…³ç³»è¾¹ç±»å‹ | ä½¿ç”¨è§„åˆ™æˆ–å¤§æ¨¡å‹åˆ¤æ–­â€œåŒ…å«â€ã€â€œå¼•ç”¨â€ã€â€œäº§å“ç›¸å…³â€ç­‰         |
| æ˜¾ç¤ºç›¸ä¼¼åº¦åˆ†æ•°  | è¾¹æ ‡ç­¾åŠ ä¸Šç›¸ä¼¼åº¦ï¼Œå¦‚ "å†…å®¹ç›¸å…³(0.82)"             |
| äº¤äº’å¼å›¾è°±    | ç”¨ pyvis / D3.js / Streamlit æ˜¾ç¤ºå¯ç‚¹å‡»å›¾è°± |
| å¤šè¯­ç§æ”¯æŒ    | å½“å‰æ¨¡å‹æ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æ¡£                       |

---

æ˜¯å¦å¸Œæœ›æˆ‘ç»§ç»­å¸®ä½ æ„å»ºäº¤äº’å¼å¯è§†åŒ–ï¼ˆå¦‚ç½‘é¡µç‚¹å‡»å±•ç¤ºï¼‰ï¼Ÿ
ä¸‹é¢ç»™å‡º **å®Œæ•´å¯è¿è¡Œçš„é¡¹ç›®ä»£ç **ï¼Œé‡ç‚¹åœ¨äºä¸¤å¤„æ”¹è¿›ï¼š

1. **å¼ºåŒ– Prompt**ï¼šè®©å¤§æ¨¡å‹ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºâ€œæ ‡ç­¾â€ï¼›
2. **å¢å¼ºè§£æå‡½æ•°**ï¼šå…¼å®¹å¤šç§è¾“å‡ºæ ¼å¼ï¼Œç¡®ä¿èƒ½æ­£ç¡®æå–æ ‡ç­¾ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„å›é¡¾

```
document_graph_project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ extract_text.py
â”‚   â”œâ”€â”€ analyze_docs.py
â”‚   â”œâ”€â”€ build_graph.py
â”‚   â””â”€â”€ export_dify.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/
â”œâ”€â”€ output/
â”‚   â””â”€â”€ graph.html
â”œâ”€â”€ run.py
â””â”€â”€ requirements.txt
```

---

## 1. `app/analyze_docs.py`ï¼ˆæ‘˜è¦+æ ‡ç­¾è§£æï¼‰

```python
from transformers import AutoTokenizer, AutoModel
import torch
import re

# åˆå§‹åŒ–æ¨¡å‹
def init_model(gpu_id=0):
    device = f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True) \
                    .half().to(device).eval()
    return tokenizer, model, device

# å°†é•¿æ–‡æœ¬æŒ‰å­—æ•°æˆ– token ç²—åˆ†
def chunk_text(text, max_len=1500):
    return [text[i:i+max_len] for i in range(0, len(text), max_len)]

# å¯¹å•ä¸ªæ–‡æ¡£è°ƒç”¨æ¨¡å‹ï¼Œè¿”å›â€œåŸå§‹å“åº”â€
def summarize_and_tag_single(args):
    fname, text, gpu_id = args
    tokenizer, model, device = init_model(gpu_id)
    chunks = chunk_text(text)
    combined = ""
    for idx, chunk in enumerate(chunks, 1):
        prompt = (
            "è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š\n"
            "ã€æ€»ç»“ã€‘è¿™é‡Œæ”¾æœ¬æ®µæ‘˜è¦æ–‡å­—\n"
            "ã€æ ‡ç­¾ã€‘æ ‡ç­¾1ã€æ ‡ç­¾2ã€æ ‡ç­¾3\n\n"
            f"æ–‡æ¡£å†…å®¹ï¼ˆç¬¬{idx}æ®µï¼‰ï¼š\n{chunk}"
        )
        response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=512)
        combined += f"\n=== æ®µè½ {idx} è¾“å‡º ===\n" + response + "\n"
    return fname, combined

# è§£ææ¨¡å‹è¿”å›æ–‡æœ¬ï¼Œæå–â€œæ‘˜è¦â€å’Œâ€œæ ‡ç­¾â€
def parse_summary_and_labels(raw_text):
    # å…ˆæŒ‰æ®µè½åˆ†å‰²
    parts = re.split(r"=== æ®µè½ \d+ è¾“å‡º ===", raw_text)
    full_summary = []
    tag_set = set()

    for part in parts:
        # æå–æ‘˜è¦
        sum_match = re.search(r"ã€æ€»ç»“ã€‘(.*?)\n", part, re.S)
        if sum_match:
            full_summary.append(sum_match.group(1).strip())
        # æå–æ ‡ç­¾
        tag_match = re.search(r"ã€æ ‡ç­¾ã€‘(.*?)\n", part, re.S)
        if tag_match:
            raw = tag_match.group(1)
            # æ”¯æŒé¡¿å·ã€é€—å·ã€ç©ºæ ¼åˆ†å‰²
            for t in re.split(r"[ã€,ï¼Œ\s]+", raw):
                t = t.strip()
                if t:
                    tag_set.add(t)

    summary = "\n".join(full_summary).strip()
    tags = list(tag_set)
    return summary, tags
```

---

## 2. `app/extract_text.py`ï¼ˆPDF æ–‡æœ¬æå–ï¼‰

```python
import fitz
import os

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    texts = []
    for page in doc:
        txt = page.get_text().strip()
        if txt:
            texts.append(txt)
    return "\n".join(texts)

def load_all_pdfs(folder):
    data = {}
    for fn in os.listdir(folder):
        if fn.lower().endswith(".pdf"):
            path = os.path.join(folder, fn)
            data[fn] = extract_text_from_pdf(path)
    return data
```

---

## 3. `app/build_graph.py`ï¼ˆäº¤äº’å¼ç½‘é¡µå¯è§†åŒ–ï¼‰

```python
import networkx as nx
from pyvis.network import Network
import os

def build_doc_graph(doc_infos, output_path="output/graph.html"):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    G = nx.Graph()
    # æ·»åŠ èŠ‚ç‚¹
    for name, info in doc_infos.items():
        G.add_node(name, title=info["summary"], label=name)
    # æ·»åŠ åŸºäºæ ‡ç­¾äº¤é›†çš„è¾¹
    names = list(doc_infos.keys())
    for i in range(len(names)):
        for j in range(i+1, len(names)):
            n1, n2 = names[i], names[j]
            tags1, tags2 = set(doc_infos[n1]["tags"]), set(doc_infos[n2]["tags"])
            common = tags1 & tags2
            if common:
                G.add_edge(n1, n2, label="ã€".join(common))
    net = Network(height="800px", width="100%", directed=False, notebook=False)
    net.from_nx(G)
    net.show_buttons(filter_=['physics'])
    net.show(output_path)
    print("å›¾è°±å·²ç”Ÿæˆ:", output_path)
```

---

## 4. `app/export_dify.py`ï¼ˆå¯¼å‡º Dify æ ¼å¼ï¼‰

```python
import json, os

def export_to_dify_format(doc_infos, output_file="output/dify_dataset.json"):
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    arr = []
    for name, info in doc_infos.items():
        arr.append({
            "id": name,
            "content": info["summary"],
            "metadata": {"tags": info["tags"]}
        })
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(arr, f, ensure_ascii=False, indent=2)
    print("Dify æ•°æ®å·²å¯¼å‡º:", output_file)
```

---

## 5. ä¸»æµç¨‹è„šæœ¬ `run.py`

```python
from app.extract_text import load_all_pdfs
from app.analyze_docs import summarize_and_tag_single, parse_summary_and_labels
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
from concurrent.futures import ProcessPoolExecutor
from tqdm import tqdm
import torch
import os

def main():
    pdf_dir = "data/pdfs"
    docs = load_all_pdfs(pdf_dir)

    # å¹¶è¡Œè°ƒç”¨å¤§æ¨¡å‹ï¼Œè‡ªåŠ¨è½® GPU
    gpu_count = max(torch.cuda.device_count(), 1)
    tasks = [(name, text, idx % gpu_count) for idx, (name, text) in enumerate(docs.items())]

    doc_infos = {}
    with ProcessPoolExecutor(max_workers=gpu_count) as exe:
        for name, raw in tqdm(exe.map(summarize_and_tag_single, tasks), total=len(tasks), desc="åˆ†ææ–‡æ¡£"):
            summary, tags = parse_summary_and_labels(raw)
            print(f"{name} â†’ æ ‡ç­¾: {tags}")
            doc_infos[name] = {"summary": summary, "tags": tags}

    # å¯è§†åŒ–ç½‘é¡µ
    build_doc_graph(doc_infos)

    # å¯¼å‡º Dify æ•°æ®
    export_to_dify_format(doc_infos)

if __name__ == "__main__":
    main()
```

---

### ğŸ”§ ä½¿ç”¨æ­¥éª¤

1. å®‰è£…ä¾èµ–ï¼š

   ```bash
   pip install transformers torch pymupdf networkx pyvis tqdm
   ```
2. å°† PDF æ”¾å…¥ `data/pdfs/`
3. æ‰§è¡Œä¸»è„šæœ¬ï¼š

   ```bash
   python run.py
   ```
4. æµè§ˆå™¨æ‰“å¼€ `output/graph.html`ï¼Œå³å¯çœ‹åˆ°äº¤äº’å¼å›¾è°±ï¼›
5. åœ¨ `output/dify_dataset.json` æŸ¥çœ‹å¯¼å‡ºçš„ Dify æ ¼å¼æ•°æ®ã€‚

---

è¿™æ ·ï¼Œ**æ¨¡å‹ä¼šä¸¥æ ¼æŒ‰æ ¼å¼è¾“å‡ºæ ‡ç­¾**ï¼Œä¹Ÿä¿è¯äº†**è§£æå‡½æ•°èƒ½æ­£ç¡®æŠ½å–ä¸å†ä¸ºç©º**ï¼Œå¹¶ä¸”æ•´ä¸ªæµç¨‹æ”¯æŒå¤š GPU å¹¶è¡ŒåŠ é€Ÿã€‚

