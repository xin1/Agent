é”™è¯¯
```
ValueError: too many values to unpack (expected 3)
```
```
from app.extract_csv import load_all_csvs
from app.analyze_docs import process_document
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
import json, os

def infer_group(name):
    # æ ¹æ®æ–‡ä»¶åè§„åˆ™æå– groupï¼Œä¾‹å¦‚â€œéƒ¨é—¨Aâ€ã€â€œä¸šåŠ¡Aâ€éƒ½å±äº group "A"
    for g in "ABCDEFGHIJKLMNOPQRSTUVWXYZ":
        if f"{g}" in name:
            return g
    return "default"

def main():
    csv_dir = "data/csvs"
    docs = load_all_csvs(csv_dir)
    done_file = "output/processed.json"
    doc_infos = {}

    if os.path.exists(done_file):
        with open(done_file, "r", encoding="utf-8") as f:
            doc_infos = json.load(f)

    for name, text in docs.items():
        if name in doc_infos:
            print(f"âœ… å·²å¤„ç†: {name}ï¼Œè·³è¿‡")
            continue

        print(f"\nğŸš€ å¤„ç†ä¸­: {name}")
        summary, tags, doc_type = process_document(text, fname=name)

        if summary:
            group = infer_group(name)  # æ–°å¢ï¼šè‡ªåŠ¨æ¨æ–­æ‰€å± group
            doc_infos[name] = {
                "summary": summary,
                "tags": tags,
                "type": doc_type,
                "group": group
            }
            with open(done_file, "w", encoding="utf-8") as f:
                json.dump(doc_infos, f, ensure_ascii=False, indent=2)
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {name}ï¼Œå¯ç¨åæ‰‹åŠ¨é‡è¯•")

    build_doc_graph(doc_infos)
    export_to_dify_format(doc_infos)

if __name__ == "__main__":
    main()

```
```
import networkx as nx
from pyvis.network import Network
import os

# å®šä¹‰æ–‡æ¡£ç±»å‹å±‚çº§é¡ºåº
TYPE_ORDER = [
    "éƒ¨é—¨å…·ä½“ä¸šåŠ¡",
    "ä¸šåŠ¡ä¸‹äº§å“åŸºç¡€çŸ¥è¯†",
    "æ“ä½œæŒ‡å¯¼ä¹¦"
]

def build_doc_graph(doc_infos, output_path="output/graph.html"):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    G = nx.DiGraph()

    # æ·»åŠ èŠ‚ç‚¹
    for name, info in doc_infos.items():
        label = f"{name}\n({info.get('type', 'æœªçŸ¥')})"
        G.add_node(name, title=info.get("summary", ""), label=label)

    # åˆ†ç»„å¤„ç†æ¯æ¡çº¿ç´¢ï¼ˆæ¯”å¦‚ Açº¿ã€Bçº¿ï¼‰
    groups = {}
    for name, info in doc_infos.items():
        group = info.get("group", "default")
        groups.setdefault(group, []).append((name, info))

    # æ ¹æ® TYPE_ORDER æ·»åŠ ç»„å†…å±‚çº§è¾¹
    for group_docs in groups.values():
        # å»ºç«‹ type â name æ˜ å°„
        type_to_name = {info["type"]: name for name, info in group_docs}
        for i in range(len(TYPE_ORDER) - 1):
            t1, t2 = TYPE_ORDER[i], TYPE_ORDER[i + 1]
            if t1 in type_to_name and t2 in type_to_name:
                G.add_edge(type_to_name[t1], type_to_name[t2], label=f"{t1} â {t2}")

    # æ·»åŠ ï¼šæ ¹æ®ç›¸åŒæ ‡ç­¾è¿æ¥æ‰€æœ‰æ–‡æ¡£ï¼ˆè·¨ç»„ä¹Ÿå¯ä»¥ï¼‰
    names = list(doc_infos.keys())
    for i in range(len(names)):
        for j in range(i + 1, len(names)):
            n1, n2 = names[i], names[j]
            tags1 = set(doc_infos[n1].get("tags", []))
            tags2 = set(doc_infos[n2].get("tags", []))
            common_tags = tags1 & tags2
            if common_tags:
                # ä¸ºé¿å…å’Œå±‚çº§è¾¹æ··æ·†ï¼Œä½¿ç”¨ä¸åŒé¢œè‰²/æ ·å¼ï¼ˆåœ¨å¯è§†åŒ–ä¸­å®šä¹‰ï¼‰
                G.add_edge(n1, n2, label="ã€".join(common_tags), color="#888888", arrows="")

    # å¯è§†åŒ–
    net = Network(height="800px", width="100%", directed=True, notebook=False)
    net.from_nx(G)
    net.show_buttons(filter_=['physics'])
    net.show(output_path, notebook=False)
    print("å›¾è°±å·²ç”Ÿæˆ:", output_path)


```
æ˜ç™½äº†ã€‚ä½ éœ€è¦åœ¨å¤§æ¨¡å‹å¤„ç†çš„åŸºç¡€ä¸Šï¼Œ**è‡ªåŠ¨è¯†åˆ«æ¯ä¸ªæ–‡æ¡£æ‰€å±çš„ groupï¼ˆå¦‚ A/Bï¼‰**ï¼Œæ„é€ å‡ºç»“æ„å¦‚ä¸‹çš„ `doc_infos`ï¼š

```python
{
    "éƒ¨é—¨A": {
        "summary": "è´Ÿè´£Aä¸šåŠ¡",
        "type": "éƒ¨é—¨å…·ä½“ä¸šåŠ¡",
        "tags": [...],
        "group": "A"
    },
    ...
}
```

---

### âœ… è§£å†³æ–¹æ¡ˆï¼š

æˆ‘ä»¬åªéœ€è¦åœ¨åŸ prompt åŸºç¡€ä¸Š **å¢åŠ ä¸€ä¸ªå­—æ®µâ€œã€å½’å±ç»„ã€‘â€**ï¼Œå¹¶ä¿®æ”¹æ­£åˆ™æå–éƒ¨åˆ†ï¼Œå®Œæ•´åŒ…å«ï¼š

* æ‘˜è¦
* æ ‡ç­¾
* ç±»å‹ï¼ˆå¦‚ï¼šéƒ¨é—¨å…·ä½“ä¸šåŠ¡ï¼‰
* å½’å±ç»„ï¼ˆå¦‚ï¼šAã€Bï¼‰

---

### âœ… ä¿®æ”¹åçš„å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

#### âœ… `process_document` å‡½æ•°ï¼š

```python
def process_document(text, fname="æ–‡æ¡£", max_input_length=6000):
    print(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡æ¡£: {fname}")
    truncated_text = text[:max_input_length]

    prompt = (
        "è¯·é˜…è¯»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼š\n"
        "ã€æ€»ç»“ã€‘ç®€æ´æ€»ç»“å…¨æ–‡æ ¸å¿ƒå†…å®¹ï¼›\n"
        "ã€æ ‡ç­¾ã€‘æå–3~5ä¸ªä¸»é¢˜ç›¸å…³æ ‡ç­¾ï¼Œä½¿ç”¨é¡¿å·æˆ–é€—å·åˆ†éš”ï¼›\n"
        "ã€ç±»å‹ã€‘ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€è´´è¿‘çš„ä¸€ä¸ªï¼š\n"
        "éƒ¨é—¨å…·ä½“ä¸šåŠ¡ã€ä¸šåŠ¡ä¸‹äº§å“åŸºç¡€çŸ¥è¯†ã€æ“ä½œæŒ‡å¯¼ä¹¦ã€å…¶ä»–ï¼›\n"
        "ã€å½’å±ç»„ã€‘è¯·æå–æœ¬æ–‡ä»¶å±äºå“ªä¸€ç»„ï¼ˆå¦‚Aã€Bã€C...ï¼‰ï¼Œå¦‚æ— æ³•åˆ¤æ–­åˆ™å¡«â€œæœªçŸ¥â€ã€‚\n\n"
        f"æ–‡æ¡£å†…å®¹å¦‚ä¸‹ï¼š\n{truncated_text}"
    )

    response = safe_chat(tokenizer, model, prompt)
    if not response:
        return None, [], "å…¶ä»–", "æœªçŸ¥"

    return parse_summary_tags_type_group(response)
```

---

#### âœ… æ–°çš„ `parse_summary_tags_type_group` å‡½æ•°ï¼š

```python
def parse_summary_tags_type_group(raw_text):
    sum_match = re.search(r"[ã€\[]æ€»ç»“[ã€‘\]]\s*(.*?)(?:\n|ã€|\[)", raw_text, re.S)
    tag_match = re.search(r"[ã€\[]æ ‡ç­¾[ã€‘\]]\s*(.*?)(?:\n|ã€|\[)", raw_text, re.S)
    type_match = re.search(
        r"[ã€\[]ç±»å‹[ã€‘\]]\s*(éƒ¨é—¨å…·ä½“ä¸šåŠ¡|ä¸šåŠ¡ä¸‹äº§å“åŸºç¡€çŸ¥è¯†|æ“ä½œæŒ‡å¯¼ä¹¦|å…¶ä»–)", raw_text)
    group_match = re.search(r"[ã€\[]å½’å±ç»„[ã€‘\]]\s*([A-Za-z0-9\u4e00-\u9fa5]+)", raw_text)

    summary = sum_match.group(1).strip() if sum_match else "æ— æ€»ç»“"
    tags = [t.strip() for t in re.split(r"[ã€,ï¼Œ\s]+", tag_match.group(1)) if t.strip()] if tag_match else ["æœªæå–"]
    doc_type = type_match.group(1).strip() if type_match else "å…¶ä»–"
    group = group_match.group(1).strip() if group_match else "æœªçŸ¥"

    print(f"âœ… è§£ææˆåŠŸï¼šç±»å‹={doc_type}ï¼Œæ ‡ç­¾={tags}ï¼Œå½’å±ç»„={group}")
    return summary, tags, doc_type, group
```

---

### âœ… ç¤ºä¾‹ç”¨æ³•ï¼š

```python
text = "æœ¬æŒ‡å—ç”±éƒ¨é—¨Aå‘å¸ƒï¼Œè¯´æ˜Aäº§å“çš„åŠŸèƒ½ä¸ä½¿ç”¨æ–¹å¼ã€‚"
summary, tags, doc_type, group = process_document(text, fname="æ“ä½œA")

doc_infos = {
    "æ“ä½œA": {
        "summary": summary,
        "tags": tags,
        "type": doc_type,
        "group": group
    }
}
```

---

### âœ… è¾“å‡ºç»“æ„ç¤ºä¾‹ï¼š

```python
{
    "ä¸šåŠ¡B": {
        "summary": "Bä¸šåŠ¡åŸºç¡€çŸ¥è¯†",
        "tags": ["Bç³»ç»Ÿ", "æ“ä½œæµç¨‹"],
        "type": "ä¸šåŠ¡ä¸‹äº§å“åŸºç¡€çŸ¥è¯†",
        "group": "B"
    }
}
```

---

å¦‚éœ€æ”¯æŒ**æ‰¹é‡ä»å¤šä¸ª CSV/PDF ä¸­æå–æ‰€æœ‰å­—æ®µå¹¶æ„å»ºå®Œæ•´ doc\_infos å­—å…¸**ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ å°è£…å¤„ç†æµç¨‹ã€‚æ˜¯å¦è¦ä¸€èµ·åšï¼Ÿ

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´é¡¹ç›®æµç¨‹ä¸ä»£ç ï¼Œè¯»å– CSV æ–‡ä»¶çš„ç¬¬ä¸€åˆ—å†…å®¹ä½œä¸ºæ–‡æ¡£å†…å®¹ï¼Œé€šè¿‡å¤§æ¨¡å‹ï¼ˆå¦‚ ChatGLM3ï¼‰æå–æ‘˜è¦ã€æ ‡ç­¾ã€ç±»å‹ï¼Œå¹¶ç”ŸæˆçŸ¥è¯†å›¾è°±å’Œ Dify æ ¼å¼å¯¼å‡ºã€‚

---

### ğŸ“ é¡¹ç›®ç»“æ„

```
project/
â”œâ”€â”€ run.py
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ extract_csv.py
â”‚   â”œâ”€â”€ analyze_docs.py
â”‚   â”œâ”€â”€ build_graph.py
â”‚   â””â”€â”€ export_dify.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ csvs/         # å­˜æ”¾CSVæ–‡ä»¶
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ processed.json
â”‚   â”œâ”€â”€ graph.html
â”‚   â””â”€â”€ dify_dataset.json
```

---

## ğŸ”§ 1. `run.py`

```python
from app.extract_csv import load_all_csvs
from app.analyze_docs import process_document
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
import json, os

def main():
    csv_dir = "data/csvs"
    docs = load_all_csvs(csv_dir)
    done_file = "output/processed.json"
    doc_infos = {}

    if os.path.exists(done_file):
        with open(done_file, "r", encoding="utf-8") as f:
            doc_infos = json.load(f)

    for name, text in docs.items():
        if name in doc_infos:
            print(f"âœ… å·²å¤„ç†: {name}ï¼Œè·³è¿‡")
            continue
        print(f"\nğŸš€ å¤„ç†ä¸­: {name}")
        summary, tags, doc_type = process_document(text, fname=name)
        if summary:
            doc_infos[name] = {"summary": summary, "tags": tags, "type": doc_type}
            with open(done_file, "w", encoding="utf-8") as f:
                json.dump(doc_infos, f, ensure_ascii=False, indent=2)
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {name}ï¼Œå¯ç¨åæ‰‹åŠ¨é‡è¯•")

    build_doc_graph(doc_infos)
    export_to_dify_format(doc_infos)

if __name__ == "__main__":
    main()
```

---

## ğŸ“„ 2. `app/extract_csv.py`

```python
import csv
import os

def extract_first_column_from_csv(csv_path):
    texts = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        for row in reader:
            if row and len(row) >= 1:
                texts.append(row[0].strip())  # åªä¿ç•™ç¬¬ä¸€åˆ—
    return "\n".join(texts)

def load_all_csvs(folder):
    data = {}
    for fn in os.listdir(folder):
        if fn.lower().endswith(".csv"):
            path = os.path.join(folder, fn)
            content = extract_first_column_from_csv(path)
            data[fn] = content
    return data

```

---

## ğŸ§  3. `app/analyze_docs.py`

```python
from transformers import AutoTokenizer, AutoModel
import torch
import time
import re

def init_model(device='cpu'):
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = model.float().to(device).eval()
    return tokenizer, model, device

def safe_chat(tokenizer, model, prompt, max_tokens=1024):
    try:
        response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=max_tokens)
        return response
    except Exception as e:
        print(f"âš ï¸ æ¨ç†å¤±è´¥: {e}")
        return None

def process_document(text, fname="æ–‡æ¡£"):
    device = "cpu"  # å¼ºåˆ¶ä½¿ç”¨CPU
    tokenizer, model, device = init_model(device)

    prompt = (
        "è¯·é˜…è¯»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼š\n"
        "ã€æ€»ç»“ã€‘ç®€æ´æ€»ç»“å…¨æ–‡æ ¸å¿ƒå†…å®¹ï¼›\n"
        "ã€æ ‡ç­¾ã€‘æå–3~5ä¸ªä¸»é¢˜ç›¸å…³æ ‡ç­¾ï¼Œä½¿ç”¨é¡¿å·æˆ–é€—å·åˆ†éš”ï¼›\n"
        "ã€ç±»å‹ã€‘ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€è´´è¿‘çš„ä¸€ä¸ªï¼šç»¼è¿°ã€æªæ–½ã€æ”¿ç­–ã€æŠ¥å‘Šã€é€šçŸ¥ã€å…¶å®ƒã€‚\n\n"
        f"æ–‡æ¡£å†…å®¹å¦‚ä¸‹ï¼š\n{text[:6000]}"
    )

    response = safe_chat(tokenizer, model, prompt)
    if not response:
        return None, [], "å…¶å®ƒ"

    return parse_summary_and_labels(response)

def parse_summary_and_labels(raw_text):
    sum_match = re.search(r"ã€æ€»ç»“ã€‘(.*?)\n", raw_text, re.S)
    tag_match = re.search(r"ã€æ ‡ç­¾ã€‘(.*?)\n", raw_text, re.S)
    type_match = re.search(r"ã€ç±»å‹ã€‘(ç»¼è¿°|æªæ–½|æ”¿ç­–|æŠ¥å‘Š|é€šçŸ¥|å…¶å®ƒ)", raw_text)

    summary = sum_match.group(1).strip() if sum_match else "æ— æ€»ç»“"
    tags = [t.strip() for t in re.split(r"[ã€,ï¼Œ\s]+", tag_match.group(1)) if t.strip()] if tag_match else ["æœªæå–"]
    doc_type = type_match.group(1).strip() if type_match else "å…¶å®ƒ"

    return summary, tags, doc_type
```

---

## ğŸŒ 4. `app/build_graph.py`

```python
import networkx as nx
from pyvis.network import Network
import os

def build_doc_graph(doc_infos, output_path="output/graph.html"):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    G = nx.Graph()

    for name, info in doc_infos.items():
        label = f"{name}\n({info.get('type', 'æœªçŸ¥')})"
        G.add_node(name, title=info["summary"], label=label)

    names = list(doc_infos.keys())
    for i in range(len(names)):
        for j in range(i+1, len(names)):
            n1, n2 = names[i], names[j]
            tags1, tags2 = set(doc_infos[n1]["tags"]), set(doc_infos[n2]["tags"])
            common = tags1 & tags2
            if common:
                G.add_edge(n1, n2, label="ã€".join(common))

    net = Network(height="800px", width="100%", directed=False, notebook=False)
    net.from_nx(G)
    net.show_buttons(filter_=['physics'])
    net.show(output_path)
    print("å›¾è°±å·²ç”Ÿæˆ:", output_path)
```

---

## ğŸ“¤ 5. `app/export_dify.py`

```python
import json, os

def export_to_dify_format(doc_infos, output_file="output/dify_dataset.json"):
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    arr = []
    for name, info in doc_infos.items():
        arr.append({
            "id": name,
            "content": info["summary"],
            "metadata": {
                "tags": info["tags"],
                "type": info.get("type", "å…¶å®ƒ")
            }
        })
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(arr, f, ensure_ascii=False, indent=2)
    print("âœ… Dify æ•°æ®å·²å¯¼å‡º:", output_file)
```

---

## âœ… è¿è¡Œæ–¹å¼

1. ç¡®ä¿å·²å®‰è£…ä¾èµ–ï¼š

```bash
pip install transformers torch pandas networkx pyvis pymupdf
```

2. å°† CSV æ–‡ä»¶æ”¾å…¥ `data/csvs/` ç›®å½•ã€‚

3. è¿è¡Œä¸»ç¨‹åºï¼š

```bash
python run.py
```

---

éœ€è¦æˆ‘æ‰“åŒ…æˆ ZIP æˆ–è¿›ä¸€æ­¥æ·»åŠ  Web é¡µé¢/æ¥å£å—ï¼Ÿ
