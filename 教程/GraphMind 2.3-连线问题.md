éå¸¸å¥½ï¼Œè¿™æ­£æ˜¯é²æ£’ç³»ç»Ÿåº”æœ‰çš„èƒ½åŠ›ï¼š**é‡åˆ°å¤±è´¥æ–‡ä»¶ä¸è·³è¿‡ï¼Œè€Œæ˜¯è‡ªåŠ¨é™çº§é‡è¯•ï¼Œå¹¶ä»å¤±è´¥ç‚¹ç»§ç»­å¤„ç†**ã€‚

---

## âœ… æ”¹è¿›ç›®æ ‡

æˆ‘ä»¬æ¥å¯¹é¡¹ç›®ä¸»é€»è¾‘ `run.py` å’Œåˆ†ææ¨¡å— `analyze_docs.py` è¿›è¡Œå¢å¼ºï¼Œå…·ä½“å®ç°ï¼š

### ğŸ§  åŠŸèƒ½ç›®æ ‡ï¼š

1. **å¤±è´¥æ–‡ä»¶ä¸è·³è¿‡**ï¼›
2. **å¤±è´¥åè‡ªåŠ¨é™çº§å¤„ç†ï¼ˆå¦‚åˆ‡å°æ®µã€ç¼©çŸ­ max\_new\_tokensã€åˆ‡æ¢ CPUï¼‰**ï¼›
3. **å¤„ç†çŠ¶æ€è®°å½•ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰**ï¼›
4. **å¤±è´¥æ–‡ä»¶å¯ä»æ–­ç‚¹ç»§ç»­é‡è¯•**ã€‚

---

## âœ… æ”¹è¿›åçš„é¡¹ç›®ç»“æ„ä¸æ ¸å¿ƒä»£ç 

ä»¥ä¸‹æ˜¯ä¸€æ¬¡æ€§æ”¹è¿›åçš„ **å…¨é¡¹ç›®ä»£ç ï¼ˆå…³é”®æ–‡ä»¶ï¼‰**ï¼Œå¯å¤åˆ¶ç›´æ¥æ›¿æ¢ä½¿ç”¨ï¼š

---

### `run.py`ï¼ˆä¸»è„šæœ¬ï¼Œæ”¯æŒå¤±è´¥é™çº§ä¸æ–­ç‚¹ç»­è·‘ï¼‰

```python
from app.extract_text import load_all_pdfs
from app.analyze_docs import process_document
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
import json, os

def main():
    pdf_dir = "data/pdfs"
    docs = load_all_pdfs(pdf_dir)
    done_file = "output/processed.json"
    doc_infos = {}

    # å¦‚æœä¹‹å‰æœ‰æˆåŠŸå¤„ç†çš„ç»“æœï¼ŒåŠ è½½å®ƒä»¬
    if os.path.exists(done_file):
        with open(done_file, "r", encoding="utf-8") as f:
            doc_infos = json.load(f)

    for name, text in docs.items():
        if name in doc_infos:
            print(f"âœ… å·²å¤„ç†: {name}ï¼Œè·³è¿‡")
            continue
        print(f"\nğŸš€ å¤„ç†ä¸­: {name}")
        summary, tags = process_document(text, fname=name)
        if summary:
            doc_infos[name] = {"summary": summary, "tags": tags}
            with open(done_file, "w", encoding="utf-8") as f:
                json.dump(doc_infos, f, ensure_ascii=False, indent=2)
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {name}ï¼Œå¯ç¨åæ‰‹åŠ¨é‡è¯•")

    build_doc_graph(doc_infos)
    export_to_dify_format(doc_infos)

if __name__ == "__main__":
    main()
```

---

### `app/analyze_docs.py`ï¼ˆå…¨é‡æ€»ç»“ã€è‡ªåŠ¨é™çº§ã€å¤±è´¥æ¢å¤ï¼‰

```python
from transformers import AutoTokenizer, AutoModel
import torch
import time
import re

# åˆå§‹åŒ–æ¨¡å‹
def init_model(device='cuda'):
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = model.half().to(device).eval()
    return tokenizer, model, device

# æ¨¡å‹æ¨ç†å°è£…ï¼Œæ”¯æŒé™çº§
def safe_chat(tokenizer, model, prompt, max_tokens=1024):
    try:
        response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=max_tokens)
        return response
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹æ¨ç†å¤±è´¥ï¼Œå°è¯•é™çº§: {e}")
        time.sleep(1)
        try:
            response, _ = model.chat(tokenizer, prompt[:3000], history=[], max_new_tokens=512)
            return response
        except Exception as e:
            print(f"â›”ï¸ é™çº§åä»å¤±è´¥: {e}")
            return None

# æ ¸å¿ƒåˆ†ææµç¨‹ï¼ˆå…¨æ–‡æ€»ç»“ + æ ‡ç­¾ï¼‰
def process_document(text, fname="æ–‡æ¡£"):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer, model, device = init_model(device)

    prompt = (
        "è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ï¼Œè¾“å‡ºå…¨æ–‡æ‘˜è¦ä¸æ ‡ç­¾ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
        "ã€æ€»ç»“ã€‘å…¨æ–‡æ‘˜è¦å†…å®¹\n"
        "ã€æ ‡ç­¾ã€‘æ ‡ç­¾1ã€æ ‡ç­¾2ã€æ ‡ç­¾3\n\n"
        f"æ–‡æ¡£å†…å®¹å¦‚ä¸‹ï¼š\n{text[:6000]}"  # æ§åˆ¶è¾“å…¥é•¿åº¦
    )

    response = safe_chat(tokenizer, model, prompt)
    if not response:
        return None, []

    return parse_summary_and_labels(response)

# è§£æå‡½æ•°
def parse_summary_and_labels(raw_text):
    sum_match = re.search(r"ã€æ€»ç»“ã€‘(.*?)\n", raw_text, re.S)
    tag_match = re.search(r"ã€æ ‡ç­¾ã€‘(.*?)\n", raw_text, re.S)
    summary = sum_match.group(1).strip() if sum_match else ""
    tags = []
    if tag_match:
        raw = tag_match.group(1)
        tags = [t.strip() for t in re.split(r"[ã€,ï¼Œ\s]+", raw) if t.strip()]
    return summary, tags
```

---

### âœ… é™„ï¼šå…¶ä½™æ–‡ä»¶å¯ä¿æŒä¸å˜

`extract_text.py`ã€`build_graph.py`ã€`export_dify.py` å‡å¯å¤ç”¨ä½ å·²æœ‰ä»£ç ï¼Œä¸éœ€æ”¹åŠ¨ã€‚

---

## âœ… è‡ªåŠ¨é™çº§å¤„ç†ç­–ç•¥æ€»ç»“

| åœºæ™¯      | é™çº§ç­–ç•¥                                     |
| ------- | ---------------------------------------- |
| æ˜¾å­˜çˆ†æ‰    | æˆªçŸ­è¾“å…¥æ–‡æœ¬ï¼ˆä¾‹å¦‚ `[:3000]`ï¼‰ã€é™ä½ `max_new_tokens` |
| æ¨¡å‹å¤±è´¥    | æ•æ‰å¼‚å¸¸ã€é‡è¯•                                  |
| æ— æ ‡ç­¾/æ— æ‘˜è¦ | å…è®¸è¿”å›ç©ºï¼Œä¸»æµç¨‹ä¸­è·³è¿‡å¤„ç†å³å¯                         |
| ä¸­æ–­åç»­è·‘   | é€šè¿‡ä¿å­˜ `processed.json` è®°å½•å·²å®Œæˆæ–‡æ¡£            |

---

## âœ… å¯é€‰è¿›é˜¶ï¼ˆæ–‡æ¡£é—´å…³ç³»åˆ¤æ–­ï¼‰

ä½ æåˆ°â€œæœ‰äº›æ˜¯ç»¼è¿°ï¼Œæœ‰äº›æ˜¯æªæ–½â€ï¼Œå¯ä»¥åŠ ä¸€æ®µæ¨¡å‹åˆ¤æ–­ç±»å‹ï¼š

åœ¨ prompt ä¸­åŠ å…¥ï¼š

```
è¯·åˆ¤æ–­è¯¥æ–‡æ¡£æ˜¯â€œç»¼è¿°æ€§æ€»ç»“â€è¿˜æ˜¯â€œå…·ä½“æªæ–½â€ï¼Œå¹¶è¾“å‡ºï¼š
ã€ç±»å‹ã€‘ç»¼è¿° / æªæ–½
```

è§£æå‡½æ•°ä¸­åŠ æå–å³å¯ã€‚æ˜¯å¦éœ€è¦æˆ‘å¸®ä½ åŠ è¿›æ¥ï¼Ÿ

---

éœ€è¦æˆ‘æ‰“åŒ…ä¸º ZIP é¡¹ç›®ç»“æ„æˆ–æ•´åˆè‡ªåŠ¨é‡è¯•æ—¥å¿—ç³»ç»Ÿä¹Ÿå¯ä»¥ï¼Œå‘Šè¯‰æˆ‘å³å¯ã€‚

å¥½çš„ï¼Œä»¥ä¸‹æ˜¯å®Œæ•´æ›´æ–°åçš„é¡¹ç›®ä»£ç ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š

* âœ… ä¸²è¡Œå¤„ç†æ–‡æ¡£ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸ï¼›
* âœ… ä½¿ç”¨ ChatGLM3 æ¨¡å‹ï¼Œå¯¹å…¨æ–‡è¿›è¡Œæ€»ç»“ä¸æ ‡ç­¾æå–ï¼›
* âœ… è‡ªåŠ¨è¯†åˆ«æ–‡æ¡£ç±»å‹ï¼ˆç»¼è¿° / æªæ–½ï¼‰ï¼›
* âœ… åˆ†ææ–‡æ¡£é—´çš„è¯­ä¹‰å…³ç³»ï¼ˆä¾‹å¦‚ï¼šæŸç»¼è¿°å¯¹åº”äº†å“ªäº›æªæ–½ï¼‰ï¼›
* âœ… æ„å»ºäº¤äº’å¼å›¾è°±ï¼›
* âœ… å¯¼å‡ºä¸º Dify æ ¼å¼ã€‚

---

## âœ… 1. é¡¹ç›®ç»“æ„

```
document_graph_project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ extract_text.py
â”‚   â”œâ”€â”€ analyze_docs.py
â”‚   â”œâ”€â”€ build_graph.py
â”‚   â””â”€â”€ export_dify.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/
â”œâ”€â”€ output/
â”‚   â””â”€â”€ graph.html
â”œâ”€â”€ run.py
â””â”€â”€ requirements.txt
```

---

## âœ… 2. `app/extract_text.py`

```python
import fitz
import os

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    texts = []
    for page in doc:
        txt = page.get_text().strip()
        if txt:
            texts.append(txt)
    return "\n".join(texts)

def load_all_pdfs(folder):
    data = {}
    for fn in os.listdir(folder):
        if fn.lower().endswith(".pdf"):
            path = os.path.join(folder, fn)
            data[fn] = extract_text_from_pdf(path)
    return data
```

---

## âœ… 3. `app/analyze_docs.py`

```python
from transformers import AutoTokenizer, AutoModel
import torch
import re

def init_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
    model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).half().to(device).eval()
    return tokenizer, model, device

def analyze_document(text):
    tokenizer, model, device = init_model()

    prompt = (
        "è¯·ä½ ä½œä¸ºç§‘ç ”åŠ©ç†ï¼Œé˜…è¯»ä»¥ä¸‹PDFå…¨æ–‡å†…å®¹ï¼Œæå–æ ¸å¿ƒæ‘˜è¦ï¼Œæ€»ç»“3-7ä¸ªé«˜è´¨é‡æ ‡ç­¾ï¼Œ"
        "å¹¶åˆ¤æ–­è¿™ç¯‡æ–‡ç« æ˜¯ã€ç»¼è¿°ã€‘è¿˜æ˜¯ã€å…·ä½“æªæ–½ã€‘ï¼Œæœ€åå°è¯•åˆ†æè¯¥æ–‡æ¡£å¯èƒ½ä¸å“ªäº›å…¶ä»–ä¸»é¢˜å­˜åœ¨å…³ç³»ã€‚\n\n"
        "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n"
        "ã€æ€»ç»“ã€‘è¿™é‡Œæ˜¯æ–‡ç« çš„æ•´ä½“æ‘˜è¦\n"
        "ã€æ ‡ç­¾ã€‘æ ‡ç­¾1ã€æ ‡ç­¾2ã€æ ‡ç­¾3\n"
        "ã€ç±»å‹ã€‘ç»¼è¿° / æªæ–½\n"
        "ã€å¯èƒ½ç›¸å…³ä¸»é¢˜ã€‘ä¸»é¢˜1ã€ä¸»é¢˜2\n\n"
        f"æ–‡æ¡£å…¨æ–‡å¦‚ä¸‹ï¼š\n{text}"
    )

    response, _ = model.chat(tokenizer, prompt, history=[], max_new_tokens=1024)
    return response

def parse_response(response):
    summary = re.search(r"ã€æ€»ç»“ã€‘(.*?)\n", response, re.S)
    tags = re.search(r"ã€æ ‡ç­¾ã€‘(.*?)\n", response, re.S)
    dtype = re.search(r"ã€ç±»å‹ã€‘(.*?)\n", response, re.S)
    related = re.search(r"ã€å¯èƒ½ç›¸å…³ä¸»é¢˜ã€‘(.*?)\n", response, re.S)

    def split_tags(s):
        return [t.strip() for t in re.split(r"[ã€,ï¼Œ\s]+", s) if t.strip()]

    return {
        "summary": summary.group(1).strip() if summary else "",
        "tags": split_tags(tags.group(1)) if tags else [],
        "type": dtype.group(1).strip() if dtype else "æœªçŸ¥",
        "related": split_tags(related.group(1)) if related else [],
    }
```

---

## âœ… 4. `app/build_graph.py`

```python
import networkx as nx
from pyvis.network import Network
import os

def build_doc_graph(doc_infos, output_path="output/graph.html"):
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    G = nx.DiGraph()

    # æ·»åŠ èŠ‚ç‚¹
    for name, info in doc_infos.items():
        G.add_node(name, title=info["summary"], label=f"{name}\n[{info['type']}]")

    # å»ºç«‹â€œæ ‡ç­¾ç›¸äº¤â€è¾¹
    names = list(doc_infos.keys())
    for i in range(len(names)):
        for j in range(len(names)):
            if i == j: continue
            n1, n2 = names[i], names[j]
            doc1, doc2 = doc_infos[n1], doc_infos[n2]

            # å¦‚æœ n1 æ˜¯ç»¼è¿°ï¼Œä¸” n2 çš„æ ‡ç­¾ä¸ n1 çš„ç›¸å…³ä¸»é¢˜æœ‰äº¤é›†
            if doc1["type"] == "ç»¼è¿°":
                if set(doc2["tags"]) & set(doc1["related"]):
                    G.add_edge(n1, n2, label="ç»¼è¿°â†’æªæ–½")

    net = Network(height="800px", width="100%", directed=True, notebook=False)
    net.from_nx(G)
    net.show_buttons(filter_=['physics'])
    net.show(output_path)
    print("å›¾è°±å·²ç”Ÿæˆ:", output_path)
```

---

## âœ… 5. `app/export_dify.py`

```python
import json, os

def export_to_dify_format(doc_infos, output_file="output/dify_dataset.json"):
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    arr = []
    for name, info in doc_infos.items():
        arr.append({
            "id": name,
            "content": info["summary"],
            "metadata": {
                "tags": info["tags"],
                "type": info["type"],
                "related_topics": info["related"]
            }
        })
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(arr, f, ensure_ascii=False, indent=2)
    print("Dify æ•°æ®å·²å¯¼å‡º:", output_file)
```

---

## âœ… 6. `run.py`

```python
from app.extract_text import load_all_pdfs
from app.analyze_docs import analyze_document, parse_response
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format
from tqdm import tqdm

def main():
    docs = load_all_pdfs("data/pdfs")
    doc_infos = {}

    for name, text in tqdm(docs.items(), desc="åˆ†ææ–‡æ¡£"):
        try:
            response = analyze_document(text)
            info = parse_response(response)
            doc_infos[name] = info
            print(f"{name} â†’ æ ‡ç­¾: {info['tags']} ç±»å‹: {info['type']}")
        except Exception as e:
            print(f"{name} åˆ†æå¤±è´¥:", e)

    build_doc_graph(doc_infos)
    export_to_dify_format(doc_infos)

if __name__ == "__main__":
    main()
```

---

## âœ… 7. `requirements.txt`

```txt
transformers
torch
pymupdf
networkx
pyvis
tqdm
```

---

## âœ… 8. ä½¿ç”¨è¯´æ˜

1. å®‰è£…ä¾èµ–ï¼š

   ```bash
   pip install -r requirements.txt
   ```

2. å°† PDF æ–‡æ¡£æ”¾å…¥ï¼š

   ```
   data/pdfs/
   ```

3. è¿è¡Œä¸»ç¨‹åºï¼š

   ```bash
   python run.py
   ```

4. è¾“å‡ºç»“æœï¼š

* å¯è§†åŒ–å›¾è°±ï¼š`output/graph.html`
* Dify æ•°æ®é›†ï¼š`output/dify_dataset.json`

---

å¦‚éœ€è¿›ä¸€æ­¥æ”¯æŒå›¾ç‰‡ã€éŸ³é¢‘ã€ç»“æ„åŒ–è¡¨æ ¼æå–ç­‰æ‰©å±•ï¼Œä¹Ÿå¯ä»¥ç»§ç»­é›†æˆã€‚æ˜¯å¦éœ€è¦æˆ‘ä¸ºæ­¤æä¾› Docker éƒ¨ç½²ç‰ˆæœ¬ï¼Ÿ
