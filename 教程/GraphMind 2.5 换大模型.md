é”™è¯¯ï¼š
```
Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.
```
æç¤ºè¯ä¿®æ”¹ï¼š
```
prompt = (
    "è¯·é˜…è¯»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå¹¶â€œä¸¥æ ¼â€æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼š\n\n"
    "ã€æ€»ç»“ã€‘è¯·ç”¨ä¸€å¥è¯ï¼ˆ30å­—å†…ï¼‰æ€»ç»“æœ¬æ–‡æ ¸å¿ƒå†…å®¹ï¼›\n"
    "ã€æ ‡ç­¾ã€‘æå–5~10ä¸ªå…³é”®è¯ï¼Œä½¿ç”¨é€—å·æˆ–é¡¿å·åˆ†éš”ï¼›\n"
    "ã€ç±»å‹ã€‘åˆ¤æ–­è¯¥æ–‡æ¡£å±äºä»¥ä¸‹å“ªä¸€å±‚çº§ç±»å‹ï¼ˆä»…é€‰æ‹©ä¸€ä¸ªï¼‰ï¼š\n"
    "1ï¼‰æ“ä½œç»†èŠ‚ï¼šå¦‚ä½œä¸šæŒ‡å¯¼ä¹¦ã€SOPã€å·¥è‰ºå‚æ•°ç­‰ï¼Œç›´æ¥æŒ‡å¯¼æ“ä½œäººå‘˜ï¼›\n"
    "2ï¼‰æ“ä½œè§„èŒƒä¸æ§åˆ¶è¦æ±‚ï¼šå¦‚FMEAã€åˆ¶ç¨‹è´¨é‡è¦æ±‚ã€æ£€éªŒè§„èŒƒï¼Œæ§åˆ¶æ“ä½œè¿‡ç¨‹è´¨é‡ï¼›\n"
    "3ï¼‰äº§å“æŠ€æœ¯èµ„æ–™ï¼šå¦‚BOMã€å…ƒå™¨ä»¶è¯´æ˜ä¹¦ã€å›¾çº¸ã€æŠ€æœ¯è§„æ ¼ï¼Œæ˜¯ç†è§£äº§å“çš„ä¾æ®ï¼›\n"
    "4ï¼‰æµç¨‹ä¸ä¸šåŠ¡æ§åˆ¶ï¼šå¦‚è®¾è®¡å˜æ›´æµç¨‹ã€è¯•äº§æµç¨‹ã€è´¨é‡é—®é¢˜å¤„ç†æµç¨‹ï¼Œè§„å®šæµç¨‹è§„åˆ™ï¼›\n"
    "5ï¼‰ç®¡ç†ä½“ç³»æ–‡ä»¶ï¼šå¦‚è´¨é‡ä½“ç³»æ‰‹å†Œã€åˆ¶åº¦ã€ç»„ç»‡èŒè´£æè¿°ï¼Œå±äºä½“ç³»å±‚é¢çš„æŒ‡å¯¼æ–‡ä»¶ï¼›\n"
    "6ï¼‰å…¶ä»–ï¼šå¦‚ä¼šè®®çºªè¦ã€æ— æ³•å½’ç±»çš„å†…å®¹ï¼›\n\n"
    "ã€å½’å±ç»„ã€‘è¯·æå–æœ¬æ–‡ä»¶æ‰€å±çš„å›¢é˜Ÿæˆ–éƒ¨é—¨åç§°ï¼ˆå¦‚äº§å“å·¥ç¨‹éƒ¨ã€åˆ¶é€ éƒ¨ã€å“ç®¡éƒ¨ï¼‰ï¼Œå¦‚æ— æ˜ç¡®æŒ‡ç¤ºï¼Œåˆ™å¡«å†™â€œæœªçŸ¥â€ã€‚\n\n"
    f"æ–‡æ¡£å†…å®¹å¦‚ä¸‹ï¼š\n{text[:6000]}"
)

```
å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„ `analyze_docs.py` æ–‡ä»¶ï¼Œä½¿ç”¨çš„æ˜¯ **DeepSeek Chat æ¨¡å‹**ï¼ˆä¾‹å¦‚ `deepseek-ai/deepseek-llm-7b-chat`ï¼‰ï¼Œå¹¶é€‚é…ä½ åŸæœ‰çš„æµç¨‹ï¼šæ–‡æ¡£å¤„ç†ã€æ€»ç»“æå–ã€æ ‡ç­¾åˆ†ç±»ã€å½’å±ç»„åˆ¤æ–­ï¼Œå…¨éƒ¨è¾“å‡ºç»“æ„åŒ–ç»“æœã€‚

---

### âœ… ä¾èµ–é¡¹å®‰è£…ï¼ˆè¯·ç¡®ä¿ä½ å·²å®‰è£…å¦‚ä¸‹ï¼‰

```bash
pip install transformers accelerate torch
```

---

### ğŸ“„ `analyze_docs.py` å®Œæ•´ä»£ç ï¼ˆé€‚é… DeepSeek Chatï¼‰

```python
# analyze_docs.py

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
import re

# åˆå§‹åŒ– DeepSeek Chat æ¨¡å‹
def init_model(device=None):
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    model_name = "deepseek-ai/deepseek-llm-7b-chat"
    
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹ {model_name} åˆ° {device} ...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16)
    model = model.to(device).eval()
    return tokenizer, model, device

# æ‰§è¡Œæ¨ç†ï¼Œå°è£…ä¸ºå®‰å…¨è°ƒç”¨
def safe_chat(tokenizer, model, prompt, max_tokens=1024, temperature=0.7):
    try:
        # æ„å»ºç¬¦åˆ DeepSeek Chat çš„ Prompt æ ¼å¼
        full_prompt = f"<|system|>\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚\n<|user|>\n{prompt}\n<|assistant|>\n"
        inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=False
            )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æˆªå– assistant éƒ¨åˆ†
        if "<|assistant|>" in response:
            response = response.split("<|assistant|>")[-1].strip()
        return response
    except Exception as e:
        print(f"âš ï¸ æ¨ç†å¤±è´¥: {e}")
        return None

# æ–‡æ¡£å¤„ç†å‡½æ•°
def process_document(text, fname="æ–‡æ¡£"):
    tokenizer, model, device = init_model()

    prompt = (
        "è¯·é˜…è¯»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå¹¶â€œä¸¥æ ¼â€æŒ‰ç…§æ ¼å¼è¾“å‡ºï¼š\n"
        "ã€æ€»ç»“ã€‘30å­—ç®€æ´æ€»ç»“å…¨æ–‡æ ¸å¿ƒå†…å®¹ï¼›\n"
        "ã€æ ‡ç­¾ã€‘æå–5~10ä¸ªä¸»é¢˜ç›¸å…³æ ‡ç­¾ï¼Œä½¿ç”¨é¡¿å·æˆ–é€—å·åˆ†éš”ï¼›\n"
        "ã€ç±»å‹ã€‘ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€è´´è¿‘çš„ä¸€ä¸ªï¼š\n"
        "éƒ¨é—¨å…·ä½“ä¸šåŠ¡ã€ä¸šåŠ¡ä¸‹äº§å“åŸºç¡€çŸ¥è¯†ã€æ“ä½œæŒ‡å¯¼ä¹¦ã€å…¶ä»–ï¼›\n"
        "ã€å½’å±ç»„ã€‘è¯·æå–æœ¬æ–‡ä»¶å±äºå“ªä¸€ç»„ï¼ˆå¦‚ä¾›åº”åˆ¶ä½œéƒ¨é—¨ã€æ˜Ÿæ˜Ÿæµ·éƒ¨é—¨ã€AIéƒ¨é—¨...ï¼‰ï¼Œå¦‚æ— æ³•åˆ¤æ–­åˆ™å¡«â€œæœªçŸ¥â€ã€‚\n\n"
        f"æ–‡æ¡£å†…å®¹å¦‚ä¸‹ï¼š\n{text[:6000]}"
    )

    response = safe_chat(tokenizer, model, prompt)
    if not response:
        return None, [], "å…¶ä»–", "æœªçŸ¥"

    return parse_summary_tags_type_group(response)

# æå–ç»“æ„åŒ–ä¿¡æ¯
def parse_summary_tags_type_group(raw_text):
    sum_match = re.search(r"[ã€\[]æ€»ç»“[ã€‘\]]\s*(.*?)(?:\n|ã€|\[)", raw_text, re.S)
    tag_match = re.search(r"[ã€\[]æ ‡ç­¾[ã€‘\]]\s*(.*?)(?:\n|ã€|\[)", raw_text, re.S)
    type_match = re.search(r"[ã€\[]ç±»å‹[ã€‘\]]\s*(éƒ¨é—¨å…·ä½“ä¸šåŠ¡|ä¸šåŠ¡ä¸‹äº§å“åŸºç¡€çŸ¥è¯†|æ“ä½œæŒ‡å¯¼ä¹¦|å…¶ä»–)", raw_text)
    group_match = re.search(r"[ã€\[]å½’å±ç»„[ã€‘\]]\s*([A-Za-z0-9\u4e00-\u9fa5]+)", raw_text)

    summary = sum_match.group(1).strip() if sum_match else "æ— æ€»ç»“"
    tags = [t.strip() for t in re.split(r"[ã€,ï¼Œ\s]+", tag_match.group(1)) if t.strip()] if tag_match else ["æœªæå–"]
    doc_type = type_match.group(1).strip() if type_match else "å…¶ä»–"
    group = group_match.group(1).strip() if group_match else "æœªçŸ¥"

    print(f"âœ… è§£ææˆåŠŸï¼šç±»å‹={doc_type}ï¼Œæ ‡ç­¾={tags}ï¼Œå½’å±ç»„={group}")
    return summary, tags, doc_type, group

# ç¤ºä¾‹å…¥å£ï¼ˆæµ‹è¯•ï¼‰
if __name__ == "__main__":
    sample_text = "æœ¬äº§å“æ˜¯æ˜Ÿæ˜Ÿæµ·éƒ¨é—¨è´Ÿè´£çš„ä¸šåŠ¡æ¨¡å—ä¹‹ä¸€ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡ä¾›åº”é“¾ç®¡ç†åœºæ™¯ï¼Œå…·æœ‰é«˜åº¦ç¨³å®šæ€§ä¸å¯æ‰©å±•æ€§ã€‚ç”¨æˆ·å¯é€šè¿‡æœ¬æ‰‹å†Œäº†è§£å…¶åŸºæœ¬é…ç½®æµç¨‹å’Œå¸¸è§æ•…éšœæ’æŸ¥æ–¹æ³•ã€‚"
    summary, tags, doc_type, group = process_document(sample_text)
    print("ğŸ“„ æ€»ç»“:", summary)
    print("ğŸ·ï¸ æ ‡ç­¾:", tags)
    print("ğŸ“˜ ç±»å‹:", doc_type)
    print("ğŸ‘¥ å½’å±ç»„:", group)
```

---

### âœ… ä½¿ç”¨è¯´æ˜ï¼š

1. æ–‡æ¡£å†…å®¹ä¼ å…¥ `process_document(text)`ï¼Œä¼šè‡ªåŠ¨è¿”å›ï¼š

   * `summary`ï¼ˆæ€»ç»“ï¼‰
   * `tags`ï¼ˆæ ‡ç­¾ï¼‰
   * `doc_type`ï¼ˆæ–‡æ¡£ç±»å‹ï¼‰
   * `group`ï¼ˆå½’å±ç»„ï¼‰

2. å¯ä»¥åœ¨ä¸»ç¨‹åºä¸­æ‰¹é‡è¯»å–æ–‡ä»¶åè°ƒç”¨è¯¥å‡½æ•°ã€‚

3. æ¨¡å‹ä¼šè‡ªåŠ¨åŠ è½½è‡³ `CUDA`ï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œå¹¶ä½¿ç”¨ `float16` åŠ å¿«æ¨ç†ã€‚

---

å¦‚ä½ è¿˜æœ‰å¤šæ–‡æ¡£æ‰¹é‡åˆ†æã€ç¼“å­˜æ¨¡å‹ã€æˆ–åŠ å…¥è¿›åº¦æ¡çš„éœ€æ±‚ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ é›†æˆã€‚éœ€è¦å—ï¼Ÿ
