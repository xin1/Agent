é”™è¯¯
```
ModuleNotFoundError: No module named 'frontend'
File "D:\Gradio\code\Graph_Mind\document_graph_project\app\extract_text.py", line 1, in <module>
    import fitz  # PyMuPDF
  File "D:\Gradio\code\.venv\lib\site-packages\fitz\__init__.py", line 1, in <module>
    from frontend import *
ModuleNotFoundError: No module named 'frontend'
```
å¥½çš„ï¼Œä»¥ä¸‹æ˜¯åŸºäºæœ¬åœ°å¤§æ¨¡å‹ï¼ˆå¦‚ [ChatGLM3](https://github.com/THUDM/ChatGLM3)ï¼‰åˆ†æå¤šä¸ª PDF æ–‡æ¡£ã€ç”Ÿæˆæ–‡æ¡£æ ‡ç­¾ä¸å…³ç³»å›¾è°±ï¼Œå¹¶å¯¼å‡ºä¸º Dify å¯è¯»æ ¼å¼çš„å®Œæ•´å¼€å‘æµç¨‹å’Œå‚è€ƒä»£ç æ¡†æ¶ï¼š

---

## ğŸ§  é¡¹ç›®ç›®æ ‡

æ„å»ºä¸€ä¸ªå¯éƒ¨ç½²çš„æœ¬åœ°ç³»ç»Ÿï¼Œå…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼š

1. **è§£æå¤šä¸ª PDF æ–‡æ¡£**ã€‚
2. **è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹æŠ½å–æ¯ç¯‡æ–‡æ¡£çš„å…³é”®ä¿¡æ¯ä¸æ ‡ç­¾**ã€‚
3. **åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§ã€æ ‡ç­¾ã€å†…å®¹ç­‰åˆ†ææ–‡æ¡£é—´çš„å…³ç³»ï¼ˆå¦‚åŒä¸€å…¬å¸ã€ç›¸ä¼¼ä¸»é¢˜ï¼‰**ã€‚
4. **ç”Ÿæˆäº¤äº’å¼çŸ¥è¯†å›¾è°±**ï¼ˆç½‘é¡µå¯è§†åŒ–ï¼Œæ‚¬åœæŸ¥çœ‹æ‘˜è¦ï¼Œæ”¯æŒæ”¾ç¼©/æ‹–åŠ¨ï¼‰ã€‚
5. **å¯¼å‡º Dify å¯è¯»çš„çŸ¥è¯†åº“æ ¼å¼ï¼ˆMarkdown + metadataï¼‰**ã€‚

---

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
document_graph_project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/                    # å­˜æ”¾å¾…å¤„ç† PDF
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ html_graph/              # ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±ç½‘é¡µ
â”‚   â””â”€â”€ dify_knowledge_base/     # Dify æ ¼å¼çš„è¾“å‡º
â”œâ”€â”€ models/
â”‚   â””â”€â”€ chatglm3/                # ä¸‹è½½çš„æœ¬åœ°æ¨¡å‹
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ extract_text.py          # PDF æ–‡å­—æŠ½å–æ¨¡å—
â”‚   â”œâ”€â”€ analyze_docs.py          # å¤§æ¨¡å‹åˆ†ææ¨¡å—
â”‚   â”œâ”€â”€ build_graph.py           # å›¾è°±ç”Ÿæˆä¸å¯è§†åŒ–æ¨¡å—
â”‚   â””â”€â”€ export_dify.py           # Dify æ ¼å¼å¯¼å‡ºæ¨¡å—
â”œâ”€â”€ requirements.txt
â””â”€â”€ run.py                       # ä¸»æµç¨‹å…¥å£
```

---

## ğŸ”§ å¼€å‘æ­¥éª¤ä¸ä»£ç æ¦‚è§ˆ

### 1ï¸âƒ£ PDF æ–‡æœ¬æŠ½å–ï¼ˆ`extract_text.py`ï¼‰

```python
import fitz  # PyMuPDF
import os

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join([page.get_text() for page in doc])

def load_all_pdfs(folder):
    texts = {}
    for fname in os.listdir(folder):
        if fname.endswith(".pdf"):
            full_path = os.path.join(folder, fname)
            texts[fname] = extract_text_from_pdf(full_path)
    return texts
```

---

### 2ï¸âƒ£ æœ¬åœ°æ¨¡å‹è°ƒç”¨ä¸æ ‡ç­¾åˆ†æï¼ˆ`analyze_docs.py`ï¼‰

ä½¿ç”¨ [ChatGLM3](https://github.com/THUDM/ChatGLM3) æœ¬åœ°è¿è¡Œï¼Œæ¨èä½¿ç”¨ Transformers æ¨ç†æ¥å£æˆ–å®˜æ–¹ WebUI API æ¨¡å¼ã€‚

```python
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).half().cuda()
model.eval()

def summarize_and_tag(doc_text):
    prompt = f"""
è¯·é˜…è¯»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œæç‚¼å…³é”®ä¿¡æ¯ï¼Œå¹¶ä¸ºå…¶æ‰“ä¸Šåˆé€‚çš„æ ‡ç­¾ï¼š
æ–‡æ¡£å†…å®¹å¦‚ä¸‹ï¼š
{doc_text[:2000]}  # æˆªæ–­é˜²æ­¢æº¢å‡º
è¾“å‡ºæ ¼å¼ï¼š
æ–‡æ¡£æ‘˜è¦ï¼š
æ ‡ç­¾ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼š
"""
    response, _ = model.chat(tokenizer, prompt, history=[])
    return response
```

---

### 3ï¸âƒ£ åˆ†ææ–‡æ¡£å…³ç³»ï¼ˆæ„å»ºå›¾è°±è¾¹ï¼Œ`build_graph.py`ï¼‰

```python
from pyvis.network import Network

def parse_summary_and_labels(raw_response):
    summary = ""
    tags = []
    for line in raw_response.splitlines():
        if line.startswith("æ–‡æ¡£æ‘˜è¦"):
            summary = line.split("ï¼š", 1)[-1].strip()
        elif line.startswith("æ ‡ç­¾"):
            tags = line.split("ï¼š", 1)[-1].split("ï¼Œ")
    return summary, [t.strip() for t in tags]

def build_doc_graph(doc_infos, output_path="outputs/html_graph/index.html"):
    net = Network(height="750px", width="100%", directed=True)
    tag_map = {}

    for fname, info in doc_infos.items():
        net.add_node(fname, label=fname, title=info['summary'])
        for tag in info['tags']:
            tag_node = f"æ ‡ç­¾:{tag}"
            tag_map.setdefault(tag_node, []).append(fname)
            net.add_node(tag_node, label=tag_node, color='orange')
            net.add_edge(tag_node, fname)

    # ç”Ÿæˆâ€œç›¸ä¼¼æ ‡ç­¾â€å…³ç³»
    for tag, files in tag_map.items():
        for i in range(len(files)):
            for j in range(i + 1, len(files)):
                net.add_edge(files[i], files[j], label="åŒæ ‡ç­¾")

    net.show(output_path)
```

---

### 4ï¸âƒ£ å¯¼å‡ºä¸º Dify å¯è¯»æ ¼å¼ï¼ˆ`export_dify.py`ï¼‰

```python
import os
import json

def export_to_dify_format(doc_infos, output_folder="outputs/dify_knowledge_base"):
    os.makedirs(output_folder, exist_ok=True)
    for fname, info in doc_infos.items():
        data = {
            "metadata": {
                "source": fname,
                "tags": info['tags'],
            },
            "content": info['summary']
        }
        out_path = os.path.join(output_folder, f"{fname}.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
```

---

### 5ï¸âƒ£ ä¸»æµç¨‹ï¼ˆ`run.py`ï¼‰

```python
from app.extract_text import load_all_pdfs
from app.analyze_docs import summarize_and_tag, parse_summary_and_labels
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format

pdf_texts = load_all_pdfs("data/pdfs")
doc_infos = {}

for fname, text in pdf_texts.items():
    raw = summarize_and_tag(text)
    summary, tags = parse_summary_and_labels(raw)
    doc_infos[fname] = {"summary": summary, "tags": tags}

build_doc_graph(doc_infos)
export_to_dify_format(doc_infos)
```

---

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

`requirements.txt` å†…å®¹ï¼š

```text
transformers
torch
pyvis
fitz  # pymupdf
```

---

## ğŸ“ è¡¥å……è¯´æ˜

* **ä½¿ç”¨ GPU åŠ é€Ÿ**ï¼šç¡®ä¿ `torch.cuda.is_available()` è¿”å› Trueã€‚
* **ç”Ÿæˆå›¾è°±äº¤äº’é¡µé¢**ï¼špyvis å¯ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ `html_graph/index.html`ã€‚
* **Dify å…¼å®¹**ï¼šè¾“å‡ºçš„ `json` æ ¼å¼å¯ä½œä¸ºæ–‡æ¡£ä¸Šä¼ è‡³ Dify çš„â€œçŸ¥è¯†åº“â€æ¨¡å—ã€‚
* **æ¨¡å‹æ›¿ä»£å»ºè®®**ï¼šå¦‚éœ€è½»é‡åŒ–æ¨¡å‹ï¼Œå¯æ›¿æ¢ä¸º MiniCPMã€Yi-1.5ã€Mistral-Chinese ç­‰ã€‚

---

æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ æ‰“åŒ…ä¸ºå¯éƒ¨ç½²çš„ Docker é¡¹ç›®ï¼Ÿæˆ–åŠ ä¸€ä¸ªç®€å•ç½‘é¡µä¸Šä¼ é¡µé¢ï¼Ÿ
