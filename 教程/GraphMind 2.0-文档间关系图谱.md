ä½ é‡åˆ°çš„é”™è¯¯æ˜¯å› ä¸º `ChatGLM3` å¹¶ä¸æ˜¯ `AutoModelForSeq2SeqLM` ç±»å‹ï¼Œè€Œæ˜¯ `AutoModel` ç±»å‹ï¼Œå¹¶ä¸æ”¯æŒæ ‡å‡†çš„ `.generate()` æ¥å£ã€‚æˆ‘ä»¬éœ€è¦ç”¨å®ƒè‡ªå·±çš„å¯¹è¯æ¥å£æ¥å®ç°å†…å®¹æ€»ç»“ä¸æ ‡ç­¾æå–ï¼ŒåŒæ—¶æ§åˆ¶è¾“å…¥é•¿åº¦ä»¥é¿å… OOM æˆ– `max_length` æŠ¥é”™ã€‚

ä»¥ä¸‹æ˜¯å®Œæ•´çš„é¡¹ç›®ç»“æ„ä¸ä¿®å¤åçš„ä»£ç æµç¨‹ï¼š

---

## âœ… é¡¹ç›®ç»“æ„

```
document_graph_project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ extract_text.py         # æå– PDF æ–‡æœ¬
â”‚   â”œâ”€â”€ analyze_docs.py         # ä½¿ç”¨ ChatGLM3 æ€»ç»“å†…å®¹å¹¶æ‰“æ ‡ç­¾
â”‚   â”œâ”€â”€ build_graph.py          # æ„å»ºçŸ¥è¯†å›¾è°±
â”‚   â””â”€â”€ export_dify.py          # å¯¼å‡º Dify æ ¼å¼
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/                   # å­˜æ”¾ PDF æ–‡æ¡£
â”œâ”€â”€ run.py                      # ä¸»è¿è¡Œè„šæœ¬
â””â”€â”€ requirements.txt
```

---

## ğŸ“¦ å®‰è£…ä¾èµ–ï¼ˆrequirements.txtï¼‰

```txt
transformers>=4.39.3
torch>=2.1.0
cpm_kernels
sentence-transformers
pymupdf
networkx
pyvis
```

---

## ğŸ§  æ¨¡å‹è°ƒç”¨éƒ¨åˆ†ï¼ˆ`app/analyze_docs.py`ï¼‰

```python
from transformers import AutoTokenizer, AutoModel
import torch

# åŠ è½½æœ¬åœ° ChatGLM3 æ¨¡å‹ï¼ˆéœ€è¦æå‰ä¸‹è½½åˆ°æœ¬åœ°ï¼‰
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True, device_map="auto").eval()

def chunk_text(text, max_tokens=2048):
    """å°†é•¿æ–‡æœ¬æŒ‰æœ€å¤§tokenæ•°åˆ‡åˆ†ä¸ºå¤šæ®µ"""
    import re
    sentences = re.split(r'(ã€‚|ï¼|\!|\.|ï¼Ÿ|\?)', text)
    chunks, current = [], ""
    for i in range(0, len(sentences), 2):
        sent = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else "")
        if len(tokenizer(current + sent).input_ids) < max_tokens:
            current += sent
        else:
            chunks.append(current)
            current = sent
    if current:
        chunks.append(current)
    return chunks

def summarize_and_tag(text):
    chunks = chunk_text(text)
    combined_summary = ""
    for i, chunk in enumerate(chunks):
        prompt = f"è¯·æ€»ç»“ä»¥ä¸‹æ–‡æ¡£å†…å®¹å¹¶æå–3-5ä¸ªæ ‡ç­¾ï¼Œè¾“å‡ºæ ¼å¼ï¼šã€æ€»ç»“ã€‘xxxã€æ ‡ç­¾ã€‘xxxï¼š\n{chunk}"
        response, _ = model.chat(tokenizer, prompt, history=[])
        combined_summary += f"\nç¬¬{i+1}æ®µï¼š{response}\n"
    return combined_summary

def parse_summary_and_labels(text):
    summary = ""
    tags = []
    if "ã€æ€»ç»“ã€‘" in text and "ã€æ ‡ç­¾ã€‘" in text:
        summary = text.split("ã€æ€»ç»“ã€‘")[1].split("ã€æ ‡ç­¾ã€‘")[0].strip()
        tag_text = text.split("ã€æ ‡ç­¾ã€‘")[1].strip()
        tags = [t.strip("ï¼Œ, ") for t in tag_text.split() if t.strip()]
    return summary, tags
```

---

## ğŸ“„ æ–‡æœ¬æå–ï¼ˆ`app/extract_text.py`ï¼‰

```python
import fitz  # PyMuPDF
import os

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join([page.get_text() for page in doc])

def load_all_pdfs(folder):
    texts = {}
    for fname in os.listdir(folder):
        if fname.endswith(".pdf"):
            full_path = os.path.join(folder, fname)
            texts[fname] = extract_text_from_pdf(full_path)
    return texts
```

---

## ğŸ§© æ„å»ºå›¾è°±ï¼ˆ`app/build_graph.py`ï¼‰

```python
import networkx as nx
from pyvis.network import Network
import os

def build_doc_graph(doc_infos, output_path="output/graph.html"):
    G = nx.Graph()
    for fname, info in doc_infos.items():
        G.add_node(fname, title=info["summary"], label=fname)

    # å…³ç³»è¾¹
    files = list(doc_infos.keys())
    for i in range(len(files)):
        for j in range(i + 1, len(files)):
            f1, f2 = files[i], files[j]
            tags1, tags2 = set(doc_infos[f1]["tags"]), set(doc_infos[f2]["tags"])
            common = tags1 & tags2
            if common:
                G.add_edge(f1, f2, label="ã€".join(common))

    # å¯è§†åŒ–
    net = Network(height="800px", width="100%", directed=False)
    net.from_nx(G)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    net.show(output_path)
```

---

## ğŸ“¤ å¯¼å‡ºä¸º Dify æ ¼å¼ï¼ˆ`app/export_dify.py`ï¼‰

```python
import json
import os

def export_to_dify_format(doc_infos, output_file="output/dify_dataset.json"):
    data = []
    for fname, info in doc_infos.items():
        data.append({
            "id": fname,
            "question": info["summary"],
            "answer": "ï¼Œ".join(info["tags"]),
        })
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
```

---

## ğŸš€ ä¸»è¿è¡Œè„šæœ¬ï¼ˆ`run.py`ï¼‰

```python
from app.extract_text import load_all_pdfs
from app.analyze_docs import summarize_and_tag, parse_summary_and_labels
from app.build_graph import build_doc_graph
from app.export_dify import export_to_dify_format

pdf_texts = load_all_pdfs("data/pdfs")
doc_infos = {}

for fname, text in pdf_texts.items():
    raw = summarize_and_tag(text)
    summary, tags = parse_summary_and_labels(raw)
    doc_infos[fname] = {"summary": summary, "tags": tags}

build_doc_graph(doc_infos)
export_to_dify_format(doc_infos)
```

---

## âœ… æœ€åè¿è¡Œè¯´æ˜

```bash
cd document_graph_project
python run.py
```

è¿è¡Œåä¼šåœ¨ `output/` ä¸‹ç”Ÿæˆï¼š

* `graph.html`ï¼šå¯äº¤äº’çŸ¥è¯†å›¾è°±
* `dify_dataset.json`ï¼šå¯ç”¨äº Dify å¯¼å…¥çš„æ•°æ®é›†

---

å¦‚æœä½ å¸Œæœ›æ·»åŠ å‰ç«¯ç½‘é¡µé¢„è§ˆï¼ˆåƒ Gradio é‚£æ ·ï¼‰ï¼Œå¯ä»¥åç»­è¡¥åŠ ã€‚æˆ‘ä¹Ÿå¯ä»¥å¸®ä½ å¯¹æ¥å‰ç«¯ã€‚å¦‚æœè¦æ”¯æŒæ‰¹é‡ä¸Šä¼ ã€å¯è§†åŒ–ã€åŒå‡»èšç„¦ç­‰åŠŸèƒ½ï¼Œä¹Ÿå¯ç»§ç»­ä¼˜åŒ–ã€‚

æ˜¯å¦éœ€è¦æˆ‘æ‰“åŒ…ä¸€ä¸ªå®Œæ•´çš„é¡¹ç›® zip æˆ–è€… Dockerfileï¼Ÿ
